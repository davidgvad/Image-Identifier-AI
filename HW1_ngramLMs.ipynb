{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidgvad/Image-Identifier-AI/blob/main/HW1_ngramLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp5tokTea0QW"
      },
      "source": [
        "# Assignment 1: N-Gram Language Models (50 Points)\n",
        "\n",
        "Authors: Kabir Ahuja, Sofia Serrano\n",
        "\n",
        "Thanks to Melissa Mitchell and Khushi Khandelwal for feedback and Kavel Rao for designing the autograder.\n",
        "\n",
        "In this project, you will implement and experiment with N-gram language models. N-gram language models are simplest versions of a language model, which make a simplifying assumption that the probability of predicting a word in a sentence only depends on the past $N$ words in the sentence. In this project you will learn:\n",
        "\n",
        "- How to train word-level unigram and N-gram LMs on text data\n",
        "- Evaluating quality of an LM by computing perplexity\n",
        "- Sampling text from an N-gram LM\n",
        "- Implement Laplace Smoothing\n",
        "- Implement Interpolation for N-Gram Language Models\n",
        "\n",
        "We will be working with Shakespeare Plays from Andrej Karpathy's [blog post on Recurrent neural networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). We also recommend going through chapter 3 of [Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/3.pdf) on N-Gram models, especially if you are not familiar with them yet."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# get any other necessary files for this project\n",
        "\n",
        "if [ ! -e \"data-needed.txt\" ]; then\n",
        "  if [ ! -e \"data_path_to_download_url.py\" ]; then\n",
        "    wget https://raw.githubusercontent.com/serrano-s/NLPassignments-students/refs/heads/main/data_path_to_download_url.py\n",
        "  else\n",
        "    echo \"data_path_to_download_url.py script already downloaded to runtime\"\n",
        "  fi\n",
        "\n",
        "  wget https://raw.githubusercontent.com/serrano-s/NLPassignments-students/refs/heads/main/assignments/NgramLanguageModels/data-needed.txt\n",
        "\n",
        "  # download all data files needed for the student-release version of this project (i.e., no hidden test files)\n",
        "  DATA_NEEDED_FILE=\"data-needed.txt\"\n",
        "  closing_slash=\"/\"\n",
        "  while IFS= read -r line; do\n",
        "    line=\"$(echo -e \"${line}\" | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//')\";\n",
        "    dirs_to_check=\"${line%${closing_slash}*}\"\n",
        "    mkdir -p $dirs_to_check\n",
        "    download_url=$(python data_path_to_download_url.py \"$line\")\n",
        "    echo $download_url;\n",
        "    wget \"$download_url\" -O \"$line\"\n",
        "  done < \"$DATA_NEEDED_FILE\"\n",
        "else\n",
        "  echo \"data-needed.txt (and presumably therefore all necessary data files) already downloaded to runtime\"\n",
        "fi\n",
        "\n",
        "if [ ! -e \"other-setup-needed.sh\" ]; then\n",
        "  wget https://raw.githubusercontent.com/serrano-s/NLPassignments-students/refs/heads/main/assignments/NgramLanguageModels/other-setup-needed.sh\n",
        "  bash other-setup-needed.sh\n",
        "  rm data_path_to_download_url.py\n",
        "else\n",
        "  echo \"other-setup-needed.sh (and presumably therefore all other necessary files) already downloaded to runtime\"\n",
        "fi"
      ],
      "metadata": {
        "id": "aifT8FCKoh_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1f66c3-ef99-4260-f3cf-7f859b515c68"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data-needed.txt (and presumably therefore all necessary data files) already downloaded to runtime\n",
            "other-setup-needed.sh (and presumably therefore all other necessary files) already downloaded to runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "DpXDtYj0a0QX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce83badb-cf8f-4ad2-ed52-0eb6f40bc8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Load necessary packages\n",
        "import os\n",
        "from typing import List, Dict\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Setup nltk\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "FACD7uBkbPs6"
      },
      "outputs": [],
      "source": [
        "# Set data directory. Important: DO NOT CHANGE THIS. MY AUTOGRADER WILL FAIL ON YOUR SUBMISSION OTHERWISE\n",
        "parent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "data_dir = os.path.join(parent_dir, \"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "HJ3c4n2Xa0QZ"
      },
      "outputs": [],
      "source": [
        "# Helper functions for sample test cases\n",
        "\n",
        "def evaluate_test_case(input, output, expected_output, output_str = \"Output\", atol=1e-4) -> Dict:\n",
        "\n",
        "    if input is not None:\n",
        "        print(\"Input:\\n\", input)\n",
        "    print(f\"{output_str}:\\n\", output)\n",
        "    print(f\"Expected {output_str}:\\n\", expected_output)\n",
        "\n",
        "    match = (\n",
        "        output == expected_output\n",
        "        if type(output) == str\n",
        "        else np.allclose(output, expected_output, atol=atol)\n",
        "    )\n",
        "\n",
        "    if match:\n",
        "        print(\"Test case passed! :)\")\n",
        "\n",
        "    else:\n",
        "        print(\"Test case failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "def evaluate_list_test_case(input: List, output: List, expected_output: List) -> Dict:\n",
        "\n",
        "    print(\"Input:\\n\", input)\n",
        "    print(\"Output:\\n\", output)\n",
        "    print(\"Expected output:\\n\", expected_output)\n",
        "    if output == expected_output:\n",
        "        print(\"Test case passed! :)\")\n",
        "\n",
        "    else:\n",
        "        print(\"Test case failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h1pNcfea0QZ"
      },
      "source": [
        "## Part 1: Word-level unigram language models (11 points)\n",
        "\n",
        "We will start by considering word-level language models i.e. language models where the smallest unit (or a unigram) that can be predicted by the model is a word. In part 1, we will implement unigram language models, which constitutes the simplest variant of N-gram models -- simply learn the distribution of each unigram (here a word) in the corpus. Recall that for a text sequence with unigrams $w_1, w_2, \\cdots, w_n$, unigram language models, the probability of the sequence is given as:\n",
        "\n",
        "$$P(w_1, w_2, \\cdots, w_n) =P(w_1)P(w_2)\\cdots P(w_n)$$\n",
        "\n",
        "where $P(w_i)$ is simply the frequency of the word $w_i$ in the training corpus.\n",
        "\n",
        "For this part we will work with the Shakespeare dataset. Let's start by loading the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "P3zVNj_za0QZ"
      },
      "outputs": [],
      "source": [
        "with open(f\"{data_dir}/shakespeare/shakespeare_train.txt\") as f:\n",
        "    train_data = f.read().split(\"\\n\")\n",
        "\n",
        "with open(f\"{data_dir}/shakespeare/shakespeare_dev.txt\") as f:\n",
        "    dev_data = f.read().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biHeGfg-a0Qa"
      },
      "source": [
        "Below we print first 10 sentences from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "mf5_xvN6a0Qa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea2b7d2-8a60-4760-8c94-d7b46998e3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen : Before we proceed any further , hear me speak .\n",
            "All : Speak , speak .\n",
            "First Citizen : You are all resolved rather to die than to famish ?\n",
            "All : Resolved .\n",
            "resolved .\n",
            "First Citizen : First , you know Caius Marcius is chief enemy to the people .\n",
            "All : We know't , we know't .\n",
            "First Citizen : Let us kill him , and we 'll have corn at our own price .\n",
            "Is't a verdict ?\n",
            "All : No more talking o n't ; let it be done : away , away !\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\".join(train_data[:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYwSzfZa0Qa"
      },
      "source": [
        "### Exercise 1.0 Text Processing\n",
        "\n",
        "Before start training our models, let's perform some basic preprocessing. For this exercise, we only want to put an \\<eos\\> tag at the end of each sentence in the training and dev sets. Implement `add_eos` function below that does that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "XcC3T-Sla0Qa",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-83e8a74c3faf2ec2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def add_eos(data: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Adds an <eos> token to the end of each line in the data.\n",
        "\n",
        "    Inputs:\n",
        "    - data: a list of strings where each string is a line of text\n",
        "\n",
        "    Returns:\n",
        "    - a list of strings where each string is a line of text with <eos> token appended\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    for line in data:\n",
        "        # remove trailing whitespace\n",
        "        line = line.rstrip()\n",
        "        # If the line is empty just return \"<eos>\" to avoid \" <eos>\"\n",
        "        if line == \"\":\n",
        "            output.append(\"<eos>\")\n",
        "        else:\n",
        "            output.append(line + \" <eos>\")\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "wSmb99Qba0Qa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c5890f-9d83-4f2f-9f72-35c4ab14acef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Case 1\n",
            "Input:\n",
            " ['hello!', 'world!']\n",
            "Output:\n",
            " ['hello! <eos>', 'world! <eos>']\n",
            "Expected output:\n",
            " ['hello! <eos>', 'world! <eos>']\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Running Sample Test Case 2\n",
            "Input:\n",
            " ['Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', 'At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', 'The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Output:\n",
            " ['Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice . <eos>', 'At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs . <eos>', 'The world was so recent that many things lacked names, and in order to indicate them it was necessary to point . <eos>']\n",
            "Expected output:\n",
            " ['Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice . <eos>', 'At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs . <eos>', 'The world was so recent that many things lacked names, and in order to indicate them it was necessary to point . <eos>']\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_add_eos():\n",
        "    print(\"Running Sample Test Case 1\")\n",
        "    data = [\"hello!\", \"world!\"]\n",
        "\n",
        "    evaluate_list_test_case(data, add_eos(data), [\"hello! <eos>\", \"world! <eos>\"])\n",
        "\n",
        "    print(\"Running Sample Test Case 2\")\n",
        "    data = [\n",
        "        \"Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "        \"At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "        \"The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\" ]\n",
        "\n",
        "    expected_output = [\n",
        "        \"Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice . <eos>\",\n",
        "        \"At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs . <eos>\",\n",
        "        \"The world was so recent that many things lacked names, and in order to indicate them it was necessary to point . <eos>\"]\n",
        "    evaluate_list_test_case(data, add_eos(data), expected_output)\n",
        "\n",
        "test_add_eos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "c9QedErya0Qb"
      },
      "outputs": [],
      "source": [
        "train_data_processed = add_eos(train_data)\n",
        "dev_data_processed = add_eos(dev_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8pqIK-Fa0Qb"
      },
      "source": [
        "### Exercise 1.1: Training (word-level) Unigram Language Model (2 Points)\n",
        "\n",
        "Training a unigram model simply corresponds to calculating frequencies of each word in the corpus, i.e.\n",
        "\n",
        "$$p(w_i) = \\frac{C(w_i)}{n}$$\n",
        "\n",
        "where $C(w_i)$ is the count of word $w_i$ in the training data and $n$ is the total number of words in the training dataset.\n",
        "\n",
        "Implement the `train_word_unigram` function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "2gjpe86Fa0Qb",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-8ae19fe766eed890",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def train_word_unigram(train_data: List[str]) -> Dict[str, float]:\n",
        "\n",
        "    \"\"\"\n",
        "    Trains a word-level unigram language model.\n",
        "\n",
        "    Inputs:\n",
        "        - train_data: List[str], list of sentences in training data\n",
        "\n",
        "    Outputs:\n",
        "        - Dict[str, float], a dictionary mapping words to their unigram probabilities\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    unigram_probs = {}\n",
        "    word_counts: Dict[str, int] = {}\n",
        "    total_tokens = 0\n",
        "\n",
        "    # count tokens\n",
        "    for sent in train_data:\n",
        "        tokens = sent.strip().split() #keeps <eos> as one token\n",
        "        for tok in tokens:\n",
        "            word_counts[tok] = word_counts.get(tok, 0) + 1\n",
        "            total_tokens += 1\n",
        "\n",
        "    # convert counts to probabilities\n",
        "    unigram_probs: Dict[str, float] = {}\n",
        "    if total_tokens == 0:\n",
        "        return unigram_probs\n",
        "\n",
        "    for tok, c in word_counts.items():\n",
        "        unigram_probs[tok] = c / total_tokens\n",
        "    return unigram_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "LKhUIFkza0Qc"
      },
      "outputs": [],
      "source": [
        "unigram_probs = train_word_unigram(train_data_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "cKDsM0zCa0Qc",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-f3e22e1c9dfc5a64",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a10662-fe24-45e0-f0f3-a5d1d3c67bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Case 1: Check if the number of unique words is correct\n",
            "Number of unique words:\n",
            " 12610\n",
            "Expected Number of unique words:\n",
            " 12610\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Running Sample Test Case 2: Check if the probability of word \"thou\" is correct\n",
            "Input:\n",
            " thou\n",
            "Output:\n",
            " 0.004559649641510829\n",
            "Expected Output:\n",
            " 0.004559649641510829\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Running Sample Test Case 3: Check if the probability of word \"love\" is correct\n",
            "Input:\n",
            " love\n",
            "Output:\n",
            " 0.0015984182127282134\n",
            "Expected Output:\n",
            " 0.0015984182127282134\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Running Sample Test Case 4: Check if the probability of word \"Richard\" is correct\n",
            "Input:\n",
            " Richard\n",
            "Output:\n",
            " 0.0005035479340675586\n",
            "Expected Output:\n",
            " 0.0005035479340675586\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Running Sample Test Case 5: Check if the probability of word \"pytorch\" is correct\n",
            "Input:\n",
            " pytorch\n",
            "Output:\n",
            " 0\n",
            "Expected Output:\n",
            " 0\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Running Sample Test Case 6: Check if the probability of word \"richard\" is correct\n",
            "Input:\n",
            " richard\n",
            "Output:\n",
            " 0\n",
            "Expected Output:\n",
            " 0\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample test cases\n",
        "def test_train_word_unigram(unigram_probs):\n",
        "\n",
        "    print(\"Running Sample Test Case 1: Check if the number of unique words is correct\")\n",
        "    evaluate_test_case(None, len(unigram_probs), 12610, output_str=\"Number of unique words\")\n",
        "\n",
        "    print(\"Running Sample Test Case 2: Check if the probability of word \\\"thou\\\" is correct\")\n",
        "    evaluate_test_case(\"thou\", unigram_probs[\"thou\"], 0.004559649641510829)\n",
        "\n",
        "    print(\"Running Sample Test Case 3: Check if the probability of word \\\"love\\\" is correct\")\n",
        "    evaluate_test_case(\"love\", unigram_probs[\"love\"], 0.0015984182127282134)\n",
        "\n",
        "    print(\"Running Sample Test Case 4: Check if the probability of word \\\"Richard\\\" is correct\")\n",
        "    evaluate_test_case(\"Richard\", unigram_probs[\"Richard\"], 0.0005035479340675586)\n",
        "\n",
        "    print(\"Running Sample Test Case 5: Check if the probability of word \\\"pytorch\\\" is correct\")\n",
        "    evaluate_test_case(\"pytorch\", unigram_probs.get(\"pytorch\", 0), 0)\n",
        "\n",
        "    print(\"Running Sample Test Case 6: Check if the probability of word \\\"richard\\\" is correct\")\n",
        "    evaluate_test_case(\"richard\", unigram_probs.get(\"richard\", 0), 0)\n",
        "\n",
        "\n",
        "test_train_word_unigram(unigram_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94dSgcAHa0Qc"
      },
      "source": [
        "### Exercise 1.2: Evaluating (word-level) Unigram Language Models using Perplexity (2 Points)\n",
        "\n",
        "Now that we have trained our first (albeit very basic) language model, our next job is to evaluate how good of a job it does in modeling the training text as well as generalizing on the unseen text. The most commonly used metric for evaluating the quality of a language model is Perplexity. Recall from the lecture, perplexity of a language model on a test dataset measures the (inverse) probability assigned by the language model to the test dataset normalized by the number of words (or tokens). Lower the perplexity the higher probability the model assigns to the text in the test dataset and hence better quality.\n",
        "\n",
        "$$\\text{perplexity}(W) = P(w_1w_2\\cdots w_n)^{\\frac{-1}{n}} = \\sqrt[n]{\\frac{1}{P(w_1w_2\\cdots w_n)}}$$\n",
        "\n",
        "where $W$ is a test set with $n$ words $w_1w_2\\cdots w_n$\n",
        "\n",
        "It is useful to do perplexity calculation in log space to avoid numerical issues\n",
        "\n",
        "$$\\text{perplexity}(W) = \\exp\\bigl(-\\frac{\\log{P(w_1w_2\\cdots w_n)}}{n}\\bigr)$$\n",
        "\n",
        "When we have multiple sentences in the corpus and we assume sentences to be independent, we can write:\n",
        "\n",
        "$$\\text{perplexity}(W) = \\exp\\bigl(-\\frac{ \\sum_{S \\in W}  \\log{P(s_1s_2\\cdots s_{n_s})}}{n}\\bigr)$$\n",
        "\n",
        "where $S$ is a sentence in the corpus $W$ with words $s_1 s_2 \\cdots s_{n_s}$ and $n_s$ is the number of words in $S$.\n",
        "\n",
        "Note that assuming sentences to be independent is something that is not actually true in practice. However, since N-gram language models are already limited in their context, it is not the greatest loss to remove dependencies between sentences. In the future homeworks we will be dropping this assumption as we build more powerful models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "E8eUgzZ-a0Qc",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-669e5cc008545af9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def eval_ppl_word_unigram(eval_data: List[str], unigram_probs: Dict[str, float]) -> float:\n",
        "\n",
        "    \"\"\"\n",
        "    Evaluates the perplexity of a word-level unigram language model on the dev set.\n",
        "\n",
        "    Inputs:\n",
        "        - dev_data: List[str], list of sentences in the evaluation data\n",
        "        - unigram_probs: Dict[str, float], a dictionary mapping words to their unigram probabilities\n",
        "\n",
        "    Outputs:\n",
        "        - float, the perplexity of the unigram language model on the evaluation set\n",
        "\n",
        "    Note 1: It is useful to do the calculations in log space and convert the final answer to original space to\n",
        "    avoid numerical issues .\n",
        "    Note 2: Assign 0 probability to words that are not in the unigram_probs dictionary, since those words were not seen during training.\n",
        "    \"\"\"\n",
        "    import math\n",
        "    perplexity = 0\n",
        "    total_log_prob = 0.0\n",
        "    total_tokens = 0\n",
        "    for sent in eval_data:\n",
        "        tokens = sent.strip().split()\n",
        "        for tok in tokens:\n",
        "            p = unigram_probs.get(tok, 0.0) # 0 for unseen words\n",
        "            if p == 0.0:\n",
        "                return float(\"inf\") # log(0) = -inf, perplexity = inf\n",
        "            total_log_prob += math.log(p) # natural log\n",
        "            total_tokens += 1\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return 1.0  #empty eval set\n",
        "\n",
        "    perplexity = math.exp(-total_log_prob / total_tokens)\n",
        "\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F26q5YDJa0Qc"
      },
      "source": [
        "First let's compute the perplexity of training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "5eiS3E8ja0Qd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7162d4-d964-41ed-b803-dfbbff8530e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "575.1110328373742\n"
          ]
        }
      ],
      "source": [
        "train_ppl = eval_ppl_word_unigram(train_data_processed, unigram_probs)\n",
        "print(train_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSfzzCHTa0Qd"
      },
      "source": [
        "You should expect a perplexity around 575 on the training data. Now let's evaluate on dev data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "bbJu9iLia0Qd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08618c7c-d655-4abe-f1e3-0a2072b0fd15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inf\n"
          ]
        }
      ],
      "source": [
        "dev_ppl = eval_ppl_word_unigram(dev_data_processed, unigram_probs)\n",
        "print(dev_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ae7xVZza0Qd"
      },
      "source": [
        "You should see a RuntimeWarning: divide by zero and a perplexity of infinity in this case. Why did this happen? Because we have some words in the dev dataset, which were never seen during training. The unigram model assigns a zero probability to those, which result in an infinite perplexity (inverse probability)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HhEVEV0a0Qd"
      },
      "source": [
        "### Exercise 1.3: Handling Unknown Words (2 Points)\n",
        "\n",
        "How do we deal with such situations? An easy solution is to introduce an unknown word token, e.g. \\<unk\\>. Before training, we replace the words which occur less than a threshold number of times with an \\<unk\\> token. E.g., if a word occurs less than 3 times in the dataset, we replace it with the \\<unk\\> token. At test time, when evaluating the perplexity if we see a word, which was unseen during training i.e. has a unigram frequency of 0, we assign that word the probability of \\<unk\\> token.\n",
        "\n",
        "Implement the `replace_rare_words_with_unks` function below which replaces words which occur less than `unk_thresh` number of times by \\<unk\\> token. Also reimplement `eval_ppl_word_unigram` to handle unseen words. You might find the [`Counter` class from the `collections` module useful](https://docs.python.org/3/library/collections.html#counter-objects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "OIZo6Dy0a0Qd",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-7335b83debed50b1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def replace_rare_words_with_unks(train_data: List[str], unk_thresh: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Replaces words that occur less than unk_thresh times in the training data with \"<unk>\" token.\n",
        "\n",
        "    Inputs:\n",
        "        - train_data: List[str], list of sentences in the training data\n",
        "        - unk_thresh: int, the threshold on the number of occurrences of a word to be considered known (less than considered <unk>)\n",
        "\n",
        "    Outputs:\n",
        "        - List[str], the training text with rare words replaced by \"<unk>\" token\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    train_data_unked = []\n",
        "    # recount token frequencies\n",
        "    all_tokens = []\n",
        "    for sent in train_data:\n",
        "        all_tokens.extend(sent.strip().split())\n",
        "    counts = Counter(all_tokens)\n",
        "    # Replace rare tokens\n",
        "    for sent in train_data:\n",
        "        tokens = sent.strip().split()\n",
        "        new_tokens = []\n",
        "        for tok in tokens:\n",
        "            if tok == \"<eos>\":\n",
        "                new_tokens.append(tok)\n",
        "            elif counts[tok] < unk_thresh:\n",
        "                new_tokens.append(\"<unk>\")\n",
        "            else:\n",
        "                new_tokens.append(tok)\n",
        "        train_data_unked.append(\" \".join(new_tokens))\n",
        "\n",
        "    return train_data_unked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "86wcwrw7a0Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22324722-9a13-477b-9568-d0789a0cc671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Testing for Threshold 3\n",
            "\n",
            "\n",
            "Sample Test Case 1: Check if the number of unique words is correct\n",
            "Number of unique words:\n",
            " 4620\n",
            "Expected Number of unique words:\n",
            " 4620\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Sample Test Case 2: Check if the probability of word \"<unk>\" is correct\n",
            "Input:\n",
            " <unk>\n",
            "Output:\n",
            " 0.045185342597383396\n",
            "Expected Output:\n",
            " 0.045185342597383396\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "# Testing for threshold 5\n",
            "\n",
            "\n",
            "Sample Test Case 3: Check if the number of unique words is correct\n",
            "Number of unique words:\n",
            " 3088\n",
            "Expected Number of unique words:\n",
            " 3088\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Sample Test Case 4: Check if the probability of word \"<unk>\" is correct\n",
            "Input:\n",
            " <unk>\n",
            "Output:\n",
            " 0.06910155961268387\n",
            "Expected Output:\n",
            " 0.06910155961268387\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample test cases\n",
        "def test_train_word_unigram_wth_unks():\n",
        "\n",
        "    print(\"# Testing for Threshold 3\\n\\n\")\n",
        "    train_data_wth_unks = replace_rare_words_with_unks(train_data_processed, 3)\n",
        "    unigram_probs_wth_unks = train_word_unigram(train_data_wth_unks)\n",
        "    print(\"Sample Test Case 1: Check if the number of unique words is correct\")\n",
        "    evaluate_test_case(None, len(unigram_probs_wth_unks), 4620, output_str=\"Number of unique words\")\n",
        "\n",
        "    print(\"Sample Test Case 2: Check if the probability of word \\\"<unk>\\\" is correct\")\n",
        "    evaluate_test_case(\"<unk>\", unigram_probs_wth_unks[\"<unk>\"], 0.045185342597383396)\n",
        "\n",
        "    print(\"# Testing for threshold 5\\n\\n\")\n",
        "    train_data_wth_unks = replace_rare_words_with_unks(train_data_processed, 5)\n",
        "    unigram_probs_wth_unks = train_word_unigram(train_data_wth_unks)\n",
        "    print(\"Sample Test Case 3: Check if the number of unique words is correct\")\n",
        "    evaluate_test_case(None, len(unigram_probs_wth_unks), 3088, output_str=\"Number of unique words\")\n",
        "\n",
        "    print(\"Sample Test Case 4: Check if the probability of word \\\"<unk>\\\" is correct\")\n",
        "    evaluate_test_case(\"<unk>\", unigram_probs_wth_unks[\"<unk>\"], 0.06910155961268387)\n",
        "\n",
        "test_train_word_unigram_wth_unks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "ohUNUEHfa0Qe"
      },
      "outputs": [],
      "source": [
        "# We will remove words that occur less than 3 times\n",
        "train_data_wth_unks = replace_rare_words_with_unks(train_data_processed, 3)\n",
        "unigram_probs_wth_unks = train_word_unigram(train_data_wth_unks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yykqyPYja0Qe"
      },
      "source": [
        "We recommend you to check the unigram probabilities before and after replacing rare words with \\<unk\\> token to verify your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "NhdUgHmza0Qe",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-7c1a766eb312e9ff",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def eval_ppl_word_unigram_with_unks(eval_data: List[str], unigram_probs_wth_unks: Dict[str, float]) -> float:\n",
        "\n",
        "    \"\"\"\n",
        "    Evaluates the perplexity of a word-level unigram language model on the dev set. For unseen words, uses the <unk> token probability.\n",
        "\n",
        "    Inputs:\n",
        "        - eval_data: string, List of sentences in eval data\n",
        "        - unigram_probs_wth_unks: Dict[str, float], a dictionary mapping words to their unigram probabilities, including <unk> token\n",
        "\n",
        "    Outputs:\n",
        "        - float, the perplexity of the unigram language model on the evaluation set\n",
        "\n",
        "    \"\"\"\n",
        "    import math\n",
        "\n",
        "    perplexity = 0\n",
        "    total_log_prob = 0.0\n",
        "    total_tokens = 0\n",
        "    unk_prob = unigram_probs_wth_unks.get(\"<unk>\", 0.0)\n",
        "\n",
        "    for sent in eval_data:\n",
        "        for tok in sent.strip().split():\n",
        "            p = unigram_probs_wth_unks.get(tok, unk_prob)\n",
        "            if p == 0.0:\n",
        "                return float(\"inf\")\n",
        "\n",
        "            total_log_prob += math.log(p)\n",
        "            total_tokens += 1\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return 1.0\n",
        "\n",
        "    perplexity=math.exp(-total_log_prob / total_tokens)\n",
        "\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHp-_Jlga0Qe"
      },
      "source": [
        "Let's compute the train and dev perplexity now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "iggwsRkaa0Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773cf899-eea2-4bd5-f50d-bf10726da677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Perplexity: 384.0815655120125\n",
            "Dev Perplexity: 282.0959070802668\n"
          ]
        }
      ],
      "source": [
        "train_ppl = eval_ppl_word_unigram_with_unks(train_data_wth_unks, unigram_probs_wth_unks)\n",
        "print(f\"Train Perplexity: {train_ppl}\")\n",
        "dev_ppl = eval_ppl_word_unigram_with_unks(dev_data_processed, unigram_probs_wth_unks)\n",
        "print(f\"Dev Perplexity: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqdsNFOWa0Qe"
      },
      "source": [
        "You should observe a train perplexity around 384 and dev perplexity around 282."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9tIZTFha0Qf"
      },
      "source": [
        "### Exercise 1.4: Sampling from Unigram Language Model (2 Points)\n",
        "\n",
        "Now that we have trained and evaluated our unigram LM, we are ready to generate some text from it. To sample text from an N-gram language model given prefix words $w_1, w_2, \\cdots, w_n$, we sequentially sample next tokens from the N-gram probability distribution given the previous words, i.e.,\n",
        "\n",
        "$$w_{n+1} \\sim P(w_{n+1} | w_{1}, \\cdots, w_{n} )$$\n",
        "\n",
        "For a unigram language model, since $P(w_1, \\dots, w_n) = P(w_1)\\cdots P(w_n)$ i.e. all words all distributed independently and the next token is sampled independent of previous tokens, the above equation simplifies to:\n",
        "\n",
        "$$w_{n+1} \\sim P(w_{n+1})$$\n",
        "\n",
        "You will now implement the function `sample_from_word_unigram` below. You might find the [Numpy's random module (np.random)](https://numpy.org/doc/stable/reference/random/index.html) useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "at0epuaga0Qf",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-08f4a7dd328533c1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def sample_from_word_unigram(unigram_probs: Dict[str, float], max_words: int, prefix: str = \"\") -> str:\n",
        "\n",
        "    \"\"\"\n",
        "    Samples sequence of words from a unigram language model.\n",
        "    Terminate sampling when either max_words is reached or when <eos> token is sampled.\n",
        "\n",
        "    Inputs:\n",
        "        - unigram_probs: Dict[str, float], a dictionary mapping words to their unigram probabilities\n",
        "        - n_words: int, the number of words to sample\n",
        "        - prefix: str, a prefix to start the sampling from. Can have multiple words separated by spaces.\n",
        "\n",
        "    Outputs:\n",
        "        - str: sampled text i.e. string of sampled words separated by spaces along with the prefix\n",
        "\n",
        "    # Note: Please use np.random.choice to sample from the unigram_probs dictionary.\n",
        "    \"\"\"\n",
        "    sampled_string = \"\"\n",
        "\n",
        "    # Convert dict\n",
        "    words = list(unigram_probs.keys())\n",
        "    probs = np.array([unigram_probs[w] for w in words], dtype=float)\n",
        "    # normalize because of floating error\n",
        "    probs = probs / probs.sum()\n",
        "    # Start with prefix\n",
        "    out_tokens = []\n",
        "    if prefix.strip() != \"\":\n",
        "        out_tokens.extend(prefix.strip().split())\n",
        "    for _ in range(max_words):\n",
        "        next_word = np.random.choice(words, p=probs)\n",
        "        if next_word == \"<eos>\":\n",
        "            break\n",
        "        out_tokens.append(next_word)\n",
        "    sampled_string = \" \".join(out_tokens)\n",
        "    return sampled_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "rVqzBMYsa0Qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da697ba6-f07d-4716-a952-dd60287692db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Test Case 1: Check if the sampled string starts with the prefix\n",
            "Sampled string starts with the prefix:\n",
            " True\n",
            "Expected Sampled string starts with the prefix:\n",
            " True\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Running Test Case 2: Check if the sampled string has either 10 generated words or ends with <eos> token\n",
            "Generated string: The king If thee For no that body of weeping drunk it\n",
            "Number of generated words: 10\n",
            "Does the generated string end with <eos> token: False\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Running Test Case 3: Check if the probability of generating <unk> token is correct\n",
            "Probability of generating <unk> token:\n",
            " 0.053180396246089674\n",
            "Expected Probability of generating <unk> token:\n",
            " 0.045185342597383396\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample test cases\n",
        "def test_sample_from_word_unigram():\n",
        "\n",
        "    np.random.seed(0)\n",
        "    sampled_string = sample_from_word_unigram(unigram_probs_wth_unks, 10, \"The king\")\n",
        "    print(\"Running Test Case 1: Check if the sampled string starts with the prefix\")\n",
        "    evaluate_test_case(None, sampled_string.startswith(\"The king\"), True, output_str=\"Sampled string starts with the prefix\")\n",
        "\n",
        "    print(\"Running Test Case 2: Check if the sampled string has either 10 generated words or ends with <eos> token\")\n",
        "    print(f\"Generated string: {sampled_string}\")\n",
        "    print(f\"Number of generated words: {len(sampled_string.split()) - 2}\")\n",
        "    print(f\"Does the generated string end with <eos> token: {'<eos>' in sampled_string}\")\n",
        "    if len(sampled_string.split()) - 2 == 10 or (\n",
        "        len(sampled_string.split()) - 2 < 10 and \"<eos>\" in sampled_string\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Running Test Case 3: Check if the probability of generating <unk> token is correct\")\n",
        "\n",
        "    sampled_strings = [\n",
        "        sample_from_word_unigram(unigram_probs_wth_unks, 1, \"\")\n",
        "        for _ in range(1000)\n",
        "    ]\n",
        "    sampled_string = \" \".join(sampled_strings)\n",
        "    num_unks = sampled_string.count(\"<unk>\")\n",
        "    unk_gen_prob = num_unks / len(sampled_string.split())\n",
        "    evaluate_test_case(None, unk_gen_prob, unigram_probs_wth_unks[\"<unk>\"], output_str=\"Probability of generating <unk> token\", atol=1e-2)\n",
        "\n",
        "test_sample_from_word_unigram()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-_c4x__a0Qf"
      },
      "source": [
        "Note that due to the randomness in sampling, it is difficult to automatically test this function. However, you can use the technique we use in sample test case 3 by repeatedly sampling from the unigram model and checking the frequency of different words in the generated text and checking if they are close to the unigram probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6dbTa5la0Qf"
      },
      "source": [
        "Let's sample some text from the unigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "s_X2FnG7a0Qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627d4c0e-d4c9-4978-f1cb-915cd29919e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "where , know this : if in <unk> do How <unk> by To OF brow God in\n",
            "Shepherd . I , KING aspiring Than our <unk> <unk> . country . the yet high : that thus Do\n",
            "I GAUNT <unk> : I <unk> willing <unk> the She thee here thou while : : 's , , None\n",
            "deed . First redemption you\n",
            "From , <unk> grandsire To his ! less . VI voice Of , thou Hold his a me So dost\n",
            ", letter : forces will The is joys <unk> feast JULIET knee what , I of are that a\n",
            "of <unk> his : queen Call The right immediately , love ANGELO wearing For . go :\n",
            "John and To you unjustly said and return , done to Not , ! <unk> he our art : A\n",
            "? <unk> thy realm the foot , A Shall\n",
            "that royal , : now of thy LADY the be I do PARIS Rome be <unk> KING . sun Henry\n"
          ]
        }
      ],
      "source": [
        "for _ in range(10):\n",
        "    sampled_string = sample_from_word_unigram(unigram_probs_wth_unks, 20)\n",
        "    print(sampled_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyhg1D0ha0Qg"
      },
      "source": [
        "You should see that the generated that doesn't make a whole lot of sense, which is natural since we are using a unigram model that doesn't account for the context at all, and essentially samples the most common tokens in its generations. We will now move to build language models that do not have this problem, i.e., they take into account the context (albeit to different degrees) for modeling the distribution of words (or tokens) in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtWblv6Za0Qg"
      },
      "source": [
        "### Write-Up Question 1: Effect of \\<unk\\> tokens on perplexity and generation quality (2 Points)\n",
        "\n",
        "What effect do you think the \\<unk\\> tokens will have on the perplexity of the model? Try out different thresholds for replacing rare words with \\<unk\\> token and report the perplexity on the training and dev set. What do you observe? Does the perplexity decrease with increasing threshold? Do you think that improves generation quality? (Your answer to this question should go in your separate write-up PDF.)\n",
        "\n",
        "**What to submit:**\n",
        "\n",
        "- A Table with the perplexity of the unigram model on the training and dev set for different thresholds of replacing rare words with \\<unk\\> token. You can choose the thresholds as 1, 3, 5,  7, 9, 10.\n",
        "\n",
        "- Example generations at each threshold.\n",
        "\n",
        "- 3-4 lines maximum discussing the trends you observe in the perplexity and generation quality with increasing threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-J9I62Vma0Qg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7rcesEQa0Qg"
      },
      "source": [
        "### Write-Up Question 2: Alterative to Random Sampling (1 Point)\n",
        "\n",
        "An alternate algorithm to generate text from a language model is greedy decoding, i.e., where we generate the most likely token at each step of decoding, i.e.\n",
        "\n",
        "$$ w_{k+1} = \\texttt{argmax}_{w} P(w | w_{1}, \\cdots, w_{k}) $$\n",
        "\n",
        "Can you explain why or why not that will be a good idea for unigram language models? Explain in no more in 3 lines. (Your answer to this question should go in your separate write-up PDF.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KckdzhrIa0Qg"
      },
      "source": [
        "## Part 2: N(>1)-Gram Word-Level Language Models (12 Points)\n",
        "\n",
        "We will now implement much more sophisticated language models, which make use of the surrounding text to model the distribution of text. Recall from the lectures for an $N$-gram language model with $n > 1$, the distribution of a sequence of tokens $w_1, w_2, \\cdots, w_n$ is given as:\n",
        "\n",
        "$$P(w_1, w_2, \\cdots, w_n) = \\prod_{k=1}^{n}P(w_k \\mid w_{k-N-1}, \\cdots, w_{k-1})$$\n",
        "\n",
        "E.g., for a bigram model i.e. $N = 2$, the expression becomes:\n",
        "\n",
        "$$P(w_1, w_2, \\cdots, w_n) = \\prod_{k=1}^{n}P(w_k \\mid w_{k-1})$$\n",
        "\n",
        "i.e. the distribution of a token depends solely on the tokens preceding it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etpnARwia0Qh"
      },
      "source": [
        "### Write-Up Question 3 (2 Points)\n",
        "\n",
        "Can you show why for an N-gram language model the following expression holds?\n",
        "\n",
        "$$P(w_1, w_2, \\cdots, w_n) = \\prod_{k=1}^{n}P(w_k \\mid w_{k-N-1}, \\cdots, w_{k-1})$$\n",
        "\n",
        "Lay down the assumption that is required to derive this expression and show all the steps in your derivation. (Your answer to this question should go in your separate write-up PDF.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XHnlidO9a0Qh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz3U2fIKa0Qh"
      },
      "source": [
        "### Exercise 2.1: Text Processing\n",
        "\n",
        "A careful reader must have noted that the above expression: $P(w_1, w_2, \\cdots, w_n) = \\prod_{k=1}^{n}P(w_k \\mid w_{k-1})$ has a problem. On the right hand side, when $k = 1$, this will result in the term $P(w_1 | w_0)$, but there is no $w_0$ in the sequence. Similarly, for a trigram language model we will have terms, $P(w_1 | w_{-1}, w_{0})$ and $P(w_2 | w_0, w_1)$ with conditionals on words that are not part of the sequence. To handle this issue, we add $N-1$ start of sequence tokens, e.g. \\<sos\\> to the beginning of the sequence.\n",
        "\n",
        "Implement `process_text_for_Ngram` function below that adds $N-1$ \\<sos\\> tokens to beginning of each sentence in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "hUj8S9qJa0Qh",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2228a409c358fa6e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def process_text_for_Ngram(sents: List[str], N: int = 2) -> List[str]:\n",
        "\n",
        "    \"\"\"\n",
        "    Adds N-1 <sos> tokens to the start of every sentence in the text.\n",
        "\n",
        "    Inputs:\n",
        "        - sents: List[str], List of sentences\n",
        "        - N: int, the N in N-gram\n",
        "\n",
        "    Outputs:\n",
        "        - List[str], the processed text\n",
        "    \"\"\"\n",
        "    processed_sents = []\n",
        "    sos_prefix = \" \".join([\"<sos>\"] * (N - 1))  # \"\" if N=1\n",
        "    for sent in sents:\n",
        "        sent = sent.strip()\n",
        "        if N <= 1:\n",
        "            processed_sents.append(sent)\n",
        "        else:\n",
        "            if sent == \"\":\n",
        "                processed_sents.append(sos_prefix)\n",
        "            else:\n",
        "                processed_sents.append(sos_prefix + \" \" + sent)\n",
        "\n",
        "    return processed_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "eJXmUNlka0Qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2384b2-b905-4565-9386-27f6fbe81506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Case 1 with N=1\n",
            "Input:\n",
            " ['Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', 'At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', 'The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Output:\n",
            " ['Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', 'At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', 'The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Expected output:\n",
            " ['Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', 'At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', 'The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Running Sample Test Case 2 with N=2\n",
            "Input:\n",
            " ['Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', 'At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', 'The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Output:\n",
            " ['<sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', '<sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', '<sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Expected output:\n",
            " ['<sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', '<sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', '<sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Running Sample Test Case 3 with N=3\n",
            "Input:\n",
            " ['Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', 'At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', 'The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Output:\n",
            " ['<sos> <sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', '<sos> <sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', '<sos> <sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Expected output:\n",
            " ['<sos> <sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', '<sos> <sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', '<sos> <sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Running Sample Test Case 4 with N=4\n",
            "Input:\n",
            " ['Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', 'At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', 'The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Output:\n",
            " ['<sos> <sos> <sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', '<sos> <sos> <sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', '<sos> <sos> <sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Expected output:\n",
            " ['<sos> <sos> <sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .', '<sos> <sos> <sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .', '<sos> <sos> <sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .']\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample test cases\n",
        "\n",
        "def test_process_text_for_Ngram():\n",
        "\n",
        "    sents = [\"Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "    \"At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "    \"The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\" ]\n",
        "\n",
        "    print(\"Running Sample Test Case 1 with N=1\")\n",
        "    unigram_processed_text = process_text_for_Ngram(sents, 1)\n",
        "    excepted_output = [\n",
        "        \"Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "        \"At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "        \"The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\"\n",
        "    ]\n",
        "    evaluate_list_test_case(sents, unigram_processed_text, excepted_output)\n",
        "\n",
        "    print(\"Running Sample Test Case 2 with N=2\")\n",
        "    bigram_processed_text = process_text_for_Ngram(sents, 2)\n",
        "    excepted_output = [\n",
        "        \"<sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "        \"<sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "        \"<sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\"\n",
        "    ]\n",
        "    evaluate_list_test_case(sents, bigram_processed_text, excepted_output)\n",
        "\n",
        "    print(\"Running Sample Test Case 3 with N=3\")\n",
        "    trigram_processed_text = process_text_for_Ngram(sents, 3)\n",
        "    excepted_output = [\n",
        "        \"<sos> <sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "        \"<sos> <sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "        \"<sos> <sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\"\n",
        "    ]\n",
        "    evaluate_list_test_case(sents, trigram_processed_text, excepted_output)\n",
        "\n",
        "    print(\"Running Sample Test Case 4 with N=4\")\n",
        "    fourgram_processed_text = process_text_for_Ngram(sents, 4)\n",
        "    excepted_output = [\n",
        "        \"<sos> <sos> <sos> Many years later , as he faced the firing squad , Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice .\",\n",
        "        \"<sos> <sos> <sos> At that time Macondo was a village of twenty adobe houses, built on the bank of a river of clear water that ran along a bed of polished stones, which were white and enormous, like prehistoric eggs .\",\n",
        "        \"<sos> <sos> <sos> The world was so recent that many things lacked names, and in order to indicate them it was necessary to point .\"\n",
        "    ]\n",
        "    evaluate_list_test_case(sents, fourgram_processed_text, excepted_output)\n",
        "\n",
        "test_process_text_for_Ngram()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6xdEYhPa0Qi"
      },
      "source": [
        "### Exercise 2.2: Implementing N-gram language models (10 Points)\n",
        "\n",
        "The heart of implementing an N-gram language model is to estimate the conditional distributions $P(w_n \\mid w_{n-N-1}, \\cdots, w_{n-1})$. Recall from the lectures that the conditional distributions can be estimated as:\n",
        "\n",
        "$$P(w_n \\mid w_{n-N-1}, \\cdots, w_{n-1}) = \\frac{C(w_{n-N-1} \\cdots w_{n-1} w_{n})}{\\sum_{w \\in V}{C(w_{n-N-1} \\cdots w_{n-1} w)}} = \\frac{C(w_{n-N-1} \\cdots w_{n-1} w_{n})}{{C(w_{n-N-1} \\cdots w_{n-1})}}$$\n",
        "\n",
        "where $C(w_{n-N-1} \\cdots w_{n-1} w)$ is the number of times the token sequence $w_{n-N-1} \\cdots w_{n-1} w$ appears in the corpus, and $V$ is the vocabulary of the N-gram model.\n",
        "\n",
        "You will now implement an N-gram language model from scratch. Note that a full implementation involves a `fit` function, which computes the N-gram counts $C(w_{n-N-1} \\cdots w_{n-1} w)$ needed to compute the conditional distributions; `eval_perplexity` function, which evaluates the perplexity of the language model on a test set; and a `sample_text` function which generates the text from the LM. Implement the class `WordNGramLM` below with these functions.This time we leave all the implementation details for you to decide. We just provide a boiler plate code, with the expected input output for the three functions in the class. You might need to implement additional functions for your implementation of the three functions.\n",
        "\n",
        "**Note 1**: If implementing a general NGram Model from scratch seems too daunting, we recommend starting implementing just the Bigram LM and checking if you are able to pass BigramLM Test cases. From there you can work on generalizing your solution.\n",
        "\n",
        "**Note 2**: Efficiency of your code will be important here, especially for `eval_perplexity` and `sample_text` functions. A naive implementation of these functions will result in a time complexity of $\\pmb{O}(nV)$ for `eval_perplexity` and $\\pmb{O}(TV)$ for `sample_text`, where $n$ is the number of words in evaluation dataset, T is the length of sampled text, and $V$ is the size of vocabulary. By caching certain quantities during training, you should be able to write implementations with time complexities $\\pmb{O}(n)$ and either $\\pmb{O}(T + V)$ or $\\pmb{O}(T)$ for the two functions. We will give only half credit for the naive implementation.\n",
        "\n",
        "**Note 3**: We will provide sample test cases only to test `eval_perplexity` and `sample_text` functions and not for the `fit` function. Both of the former functions rely on the latter to be implemented correctly, hence use correctness of `eval_perplexity` and `sample_text` to check the correctness of your `fit` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "H7YHarZha0Qi",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-cf47ebd6c0f86836",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import math\n",
        "class WordNGramLM:\n",
        "\n",
        "    def __init__(self, N: int):\n",
        "        self.N = N\n",
        "        self.vocab = set()\n",
        "\n",
        "        # Counts\n",
        "        self.context_counts = defaultdict(int) # context count\n",
        "        self.next_counts = defaultdict(lambda: defaultdict(int))  # context - word -  count\n",
        "\n",
        "        # Cached for fast sampling\n",
        "        self._sample_cache = {}\n",
        "\n",
        "    def fit(self, train_data: List[str]):\n",
        "\n",
        "        \"\"\"\n",
        "        Trains an N-gram language model.\n",
        "\n",
        "        Inputs:\n",
        "            - train_data: str, sentences in the training data\n",
        "\n",
        "        \"\"\"\n",
        "        # First Pass count all words\n",
        "        word_counts = defaultdict(int)\n",
        "        for sent in train_data:\n",
        "            # FIX: Manually append <eos> to raw data\n",
        "            sent_with_eos = sent.strip() + \" <eos>\"\n",
        "            for tok in sent_with_eos.split():\n",
        "                word_counts[tok] += 1\n",
        "\n",
        "        # Define the vocabulary freq>=3\n",
        "        self.vocab = set()\n",
        "        self.vocab.add(\"<unk>\")\n",
        "        self.vocab.add(\"<eos>\")\n",
        "        if self.N > 1:\n",
        "            self.vocab.add(\"<sos>\")\n",
        "\n",
        "        for w, c in word_counts.items():\n",
        "            if c >= 3:\n",
        "                self.vocab.add(w)\n",
        "        # Reset vounts\n",
        "        self.context_counts = defaultdict(int)\n",
        "        self.next_counts = defaultdict(lambda: defaultdict(int))\n",
        "        k = self.N - 1\n",
        "\n",
        "        # Second pass, train\n",
        "        for sent in train_data:\n",
        "            sent_with_eos = sent.strip() + \" <eos>\"\n",
        "            raw_tokens = sent_with_eos.split()\n",
        "            # Map rare to <unk>\n",
        "            tokens = []\n",
        "            for tok in raw_tokens:\n",
        "                if tok in self.vocab:\n",
        "                    tokens.append(tok)\n",
        "                else:\n",
        "                    tokens.append(\"<unk>\")\n",
        "            # Add <sos> padding\n",
        "            tokens = self._ensure_sos(tokens)\n",
        "            if self.N == 1:\n",
        "                ctx = tuple()\n",
        "                for w in tokens:\n",
        "                    self.context_counts[ctx] += 1\n",
        "                    self.next_counts[ctx][w] += 1\n",
        "            else:\n",
        "                for i in range(k, len(tokens)):\n",
        "                    ctx = tuple(tokens[i-k:i])\n",
        "                    w = tokens[i]\n",
        "                    self.context_counts[ctx] += 1\n",
        "                    self.next_counts[ctx][w] += 1\n",
        "\n",
        "        # Cache sampling distributions\n",
        "        self._sample_cache = {}\n",
        "        for ctx, w_counts in self.next_counts.items():\n",
        "            words = np.array(list(w_counts.keys()), dtype=object)\n",
        "            counts = np.array([w_counts[w] for w in words], dtype=float)\n",
        "            probs = counts / counts.sum()\n",
        "            self._sample_cache[ctx] = (words, probs)\n",
        "\n",
        "    def eval_perplexity(self, eval_data: List[str]) -> float:\n",
        "\n",
        "        \"\"\"\n",
        "        Evaluates the perplexity of the N-gram language model on the eval set.\n",
        "\n",
        "        Input:\n",
        "            - eval_data: List[str], the evaluation text\n",
        "\n",
        "        Output:\n",
        "            - float, the perplexity of the model on the evaluation set\n",
        "\n",
        "        Note : For words that are not in the vocabulary, replace them with the <unk> token.\n",
        "        Note : Don't count the <sos> tokens in your number of total tokens in order to match expected perplexities.\n",
        "\n",
        "        \"\"\"\n",
        "        total_log_prob = 0.0\n",
        "        total_tokens = 0\n",
        "        k = self.N - 1\n",
        "        for sent in eval_data:\n",
        "            # Manually append <eos> to eval data\n",
        "            sent_with_eos = sent.strip() + \" <eos>\"\n",
        "            raw_tokens = sent_with_eos.split()\n",
        "            # Add sos padding\n",
        "            raw_tokens = self._ensure_sos(raw_tokens)\n",
        "            # Map OOV to <unk>\n",
        "            tokens = []\n",
        "            for t in raw_tokens:\n",
        "                if t == \"<sos>\":\n",
        "                    tokens.append(t)\n",
        "                else:\n",
        "                    tokens.append(self._map_oov_to_unk(t))\n",
        "            if self.N == 1:\n",
        "                ctx = tuple()\n",
        "                for w in tokens:\n",
        "                    num = self.next_counts[ctx].get(w, 0)\n",
        "                    den = self.context_counts.get(ctx, 0)\n",
        "                    if num == 0 or den == 0:\n",
        "                        return float(\"inf\")\n",
        "                    total_log_prob += math.log(num) - math.log(den)\n",
        "                    if w != \"<sos>\":\n",
        "                        total_tokens += 1\n",
        "            else:\n",
        "                for i in range(k, len(tokens)):\n",
        "                    ctx = tuple(tokens[i-k:i])\n",
        "                    w = tokens[i]\n",
        "                    den = self.context_counts.get(ctx, 0)\n",
        "                    num = self.next_counts[ctx].get(w, 0)\n",
        "                    if num == 0 or den == 0:\n",
        "                        return float(\"inf\")\n",
        "                    total_log_prob += math.log(num) - math.log(den)\n",
        "                    if w != \"<sos>\":\n",
        "                        total_tokens += 1\n",
        "\n",
        "        if total_tokens == 0:\n",
        "            return 1.0\n",
        "\n",
        "        return math.exp(-total_log_prob / total_tokens)\n",
        "\n",
        "    def sample_text(self, prefix: str = \"<sos>\", max_words: int = 100) -> float:\n",
        "\n",
        "        \"\"\"\n",
        "        Samples text from the N-gram language model.\n",
        "        Terminate sampling when either max_words is reached or when <eos> token is sampled.\n",
        "        Inputs:\n",
        "            - prefix: str, the prefix to start the sampling from. Can also be multiple words separated by spaces.\n",
        "            - max_words: int, the maximum number of words to sample\n",
        "\n",
        "        Outputs:\n",
        "            - str, the sampled text\n",
        "\n",
        "        Note: Please use np.random.choice for sampling next words\n",
        "        \"\"\"\n",
        "        if prefix.strip():\n",
        "            prefix_tokens = prefix.strip().split()\n",
        "        else:\n",
        "            prefix_tokens = []\n",
        "\n",
        "        prefix_tokens = self._ensure_sos(prefix_tokens)\n",
        "\n",
        "        # map OOV in prefix to <unk> except for <sos>\n",
        "        prefix_tokens = [t if t == \"<sos>\" else self._map_oov_to_unk(t) for t in prefix_tokens]\n",
        "        out = list(prefix_tokens)\n",
        "        k = self.N - 1\n",
        "\n",
        "        for _ in range(max_words):\n",
        "            if self.N == 1:\n",
        "                ctx = tuple()\n",
        "            else:\n",
        "                ctx = tuple(out[-k:])\n",
        "\n",
        "            if ctx not in self._sample_cache:\n",
        "                # If a context was never seen in training, we cant sample from it so stop\n",
        "                break\n",
        "            words, probs = self._sample_cache[ctx]\n",
        "            nxt = np.random.choice(words, p=probs)\n",
        "            out.append(nxt)\n",
        "            if nxt == \"<eos>\":\n",
        "                break\n",
        "        return \" \".join(out)\n",
        "\n",
        "    # Extra utility functions that you think will be useful can go below\n",
        "\n",
        "    def _ensure_sos(self, tokens: List[str]) -> List[str]:\n",
        "        k = self.N - 1\n",
        "        if k <= 0:\n",
        "            return tokens\n",
        "        lead = 0\n",
        "        while lead < len(tokens) and tokens[lead] == \"<sos>\":\n",
        "            lead += 1\n",
        "        tokens_wo_extra = tokens[lead:]\n",
        "        return ([\"<sos>\"] * k) + tokens_wo_extra\n",
        "\n",
        "    def _map_oov_to_unk(self, tok: str) -> str:\n",
        "        return tok if tok in self.vocab else \"<unk>\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3-iy528mm990"
      },
      "outputs": [],
      "source": [
        "# <NO_AUTOGRADE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGuUcQRqa0Qi"
      },
      "source": [
        "As a sanity check, check if your code returns same perplexities when N=1 as your earlier unigram model perplexities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "V5nuPdtVa0Qi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39611ca3-05bf-4138-d962-57ebf9c5e77e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Perplexity for Unigram model (Class Implementation): 384.0815655120125\n",
            "Dev Perplexity for Unigram model (Class Implementation): 282.0959070802668\n",
            "Train Perplexity for Unigram model (Original Implementation): 384.0815655120125\n",
            "Dev Perplexity for Unigram model (Original Implementation): 282.0959070802668\n",
            "Unigram model perplexities match! :)\n"
          ]
        }
      ],
      "source": [
        "unigram_lm = WordNGramLM(1)\n",
        "unigram_lm.fit(train_data)\n",
        "\n",
        "train_ppl = unigram_lm.eval_perplexity(train_data)\n",
        "dev_ppl = unigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for Unigram model (Class Implementation): {train_ppl}\")\n",
        "print(f\"Dev Perplexity for Unigram model (Class Implementation): {dev_ppl}\")\n",
        "\n",
        "train_ppl_old = eval_ppl_word_unigram_with_unks(train_data_wth_unks, unigram_probs_wth_unks)\n",
        "dev_ppl_old = eval_ppl_word_unigram_with_unks(dev_data_processed, unigram_probs_wth_unks)\n",
        "\n",
        "print(f\"Train Perplexity for Unigram model (Original Implementation): {train_ppl_old}\")\n",
        "print(f\"Dev Perplexity for Unigram model (Original Implementation): {dev_ppl_old}\")\n",
        "\n",
        "if np.allclose(train_ppl, train_ppl_old, atol = 1e-4) and np.allclose(dev_ppl, dev_ppl_old, atol = 1e-4):\n",
        "    print(\"Unigram model perplexities match! :)\")\n",
        "\n",
        "else:\n",
        "    print(\"Unigram model perplexities do not match! :(\")\n",
        "\n",
        "# assert np.allclose(train_ppl, train_ppl_old, atol = 1e-4)\n",
        "# assert np.allclose(dev_ppl, dev_ppl_old, atol = 1e-4)\n",
        "# print(\"Unigram model perplexities match!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhpeS8iKa0Qi"
      },
      "source": [
        "Test implementation of `eval_perplexity` for bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "vzAV8eaxa0Qj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a44d492-c590-4ffa-fba7-7faf9f4dd4da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Perplexity: 42.128332262041745\n",
            "Dev Perplexity: inf\n"
          ]
        }
      ],
      "source": [
        "# Test implementation of `eval_perplexity` for bigram model\n",
        "bigram_lm = WordNGramLM(2)\n",
        "bigram_lm.fit(train_data)\n",
        "train_ppl = bigram_lm.eval_perplexity(train_data)\n",
        "print(f\"Train Perplexity: {train_ppl}\")\n",
        "dev_ppl = bigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Dev Perplexity: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcYXlZwba0Qj"
      },
      "source": [
        "You should see a train perplexity of roughly 42 and dev perplexity infinite, we will soon see how to deal with that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNM5CenUa0Qj"
      },
      "source": [
        "Test implementation of `sample_text` for bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "nW87Z9h0a0Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6446ce66-c487-4302-9c88-d2e74914330a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for Trigram model\n",
            "Test Case 1: Check if the sampled text starts with <sos> <sos>\n",
            "Sampled text starts with <sos> <sos>:\n",
            " True\n",
            "Expected Sampled text starts with <sos> <sos>:\n",
            " True\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\n",
            "Generated text: <sos> <sos> Please your honour , state and seat is up on high ; Whilst you have been i ' the part I had a Harry , Harry , <unk> to you both ! <eos>\n",
            "Number of generated words: 33\n",
            "Does the generated text end with <eos>: True\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 3: Check if the probability of generating II is greater than III when prefix is KING RICHARD\n",
            "Probability of generating Richard II: 0.4052\n",
            "Probability of generating Richard III: 0.5948\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 4: Check if the probability of generating II given KING RICHARD are close to the expected values\n",
            "Input:\n",
            " King Richard II\n",
            "Probability of generating Richard II:\n",
            " 0.4052\n",
            "Expected Probability of generating Richard II:\n",
            " 0.4051\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 5: Check if the probability of generating III given KING RICHARD are close to the expected values\n",
            "Input:\n",
            " King Richard III\n",
            "Probability of generating Richard III:\n",
            " 0.5948\n",
            "Expected Probability of generating Richard III:\n",
            " 0.5949\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Testing for 4-gram model\n",
            "Test Case 6: Check if the sampled text starts with <sos> <sos> <sos>\n",
            "Sampled text starts with <sos> <sos> <sos>:\n",
            " True\n",
            "Expected Sampled text starts with <sos> <sos> <sos>:\n",
            " True\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 7: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\n",
            "Generated text: <sos> <sos> <sos> Please your ladyship To visit the next room , I 'll write you down : The which , how far off lies your power ? <eos>\n",
            "Number of generated words: 26\n",
            "Does the generated text end with <eos>: True\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 8: Check if the probability of generating II is greater than III when prefix is <sos> KING RICHARD\n",
            "Probability of generating Richard II: 0.4045\n",
            "Probability of generating Richard III: 0.5955\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 9: Check if the probability of generating II given <sos> KING RICHARD are close to the expected values\n",
            "Input:\n",
            " <sos> King Richard II\n",
            "Probability of generating Richard II:\n",
            " 0.4045\n",
            "Expected Probability of generating Richard II:\n",
            " 0.4044\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 10: Check if the probability of generating III given <sos> KING RICHARD are close to the expected values\n",
            "Input:\n",
            " <sos> King Richard III\n",
            "Probability of generating Richard III:\n",
            " 0.5955\n",
            "Expected Probability of generating Richard III:\n",
            " 0.5956\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_sample_text_ngram():\n",
        "\n",
        "    print(\"Testing for Trigram model\")\n",
        "\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    trigram_lm = WordNGramLM(3)\n",
        "    trigram_lm.fit(train_data)\n",
        "    sampled_text = trigram_lm.sample_text(\"<sos> <sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 1: Check if the sampled text starts with <sos> <sos>\")\n",
        "    evaluate_test_case(None, sampled_text.startswith(\"<sos> <sos>\"), True, output_str=\"Sampled text starts with <sos> <sos>\")\n",
        "\n",
        "    print(\"Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\")\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 2}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 2 == 50 or (\n",
        "        len(sampled_text.split()) - 2 < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Test Case 3: Check if the probability of generating II is greater than III when prefix is KING RICHARD\")\n",
        "    sampled_texts = [\n",
        "        trigram_lm.sample_text(\"KING RICHARD\", max_words=1) for _ in range(10000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"KING RICHARD\")[1].strip() == \"II\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    num_richard_3s = [\n",
        "        text.split(\"KING RICHARD\")[1].strip() == \"III\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    gen_prob_richard_2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_richard_3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_richard_2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_richard_3}\")\n",
        "    if gen_prob_richard_2 < gen_prob_richard_3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Test Case 4: Check if the probability of generating II given KING RICHARD are close to the expected values\")\n",
        "    evaluate_test_case(\"King Richard II\", gen_prob_richard_2, 0.4051, output_str=\"Probability of generating Richard II\", atol=1e-2)\n",
        "\n",
        "    print(\"Test Case 5: Check if the probability of generating III given KING RICHARD are close to the expected values\")\n",
        "    evaluate_test_case(\"King Richard III\", gen_prob_richard_3, 0.5949, output_str=\"Probability of generating Richard III\", atol=1e-2)\n",
        "\n",
        "    print(\"Testing for 4-gram model\")\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    fourgram_lm = WordNGramLM(4)\n",
        "    fourgram_lm.fit(train_data)\n",
        "    # sampled_text = fourgram_lm.sample_text(\"<sos> <sos>\", max_words=50)\n",
        "\n",
        "    sampled_text = fourgram_lm.sample_text(\"<sos> <sos> <sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 6: Check if the sampled text starts with <sos> <sos> <sos>\")\n",
        "    evaluate_test_case(None, sampled_text.startswith(\"<sos> <sos> <sos>\"), True, output_str=\"Sampled text starts with <sos> <sos> <sos>\")\n",
        "\n",
        "    print(\"Test Case 7: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\")\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 3}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 3 == 50 or (\n",
        "        len(sampled_text.split()) - 3 < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Test Case 8: Check if the probability of generating II is greater than III when prefix is <sos> KING RICHARD\")\n",
        "    sampled_texts = [\n",
        "        fourgram_lm.sample_text(\"<sos> <sos> KING RICHARD\", max_words=1) for _ in range(10000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"<sos> <sos> KING RICHARD\")[1].strip() == \"II\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    num_richard_3s = [\n",
        "        text.split(\"<sos> <sos> KING RICHARD\")[1].strip() == \"III\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    gen_prob_rich2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_rich3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_rich2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_rich3}\")\n",
        "    if gen_prob_rich2 < gen_prob_rich3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Test Case 9: Check if the probability of generating II given <sos> KING RICHARD are close to the expected values\")\n",
        "    evaluate_test_case(\"<sos> King Richard II\", gen_prob_rich2, 0.4044, output_str=\"Probability of generating Richard II\", atol=1e-2)\n",
        "\n",
        "    print(\"Test Case 10: Check if the probability of generating III given <sos> KING RICHARD are close to the expected values\")\n",
        "    evaluate_test_case(\"<sos> King Richard III\", gen_prob_rich3, 0.5956, output_str=\"Probability of generating Richard III\", atol=1e-2)\n",
        "\n",
        "test_sample_text_ngram()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "iEFfpoDza0Qj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d424f9-460e-4d48-ee3c-5470f3cb37fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Case 1: Check if the sampled text starts with <sos>\n",
            "Sampled text starts with <sos>:\n",
            " True\n",
            "Expected Sampled text starts with <sos>:\n",
            " True\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\n",
            "Generated text: <sos> Please but great and the <unk> ? <eos>\n",
            "Number of generated words: 8\n",
            "Does the generated text end with <eos>: True\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 3: Check if the probability of generating II is greater than III when prefix is RICHARD\n",
            "Probability of generating Richard II: 0.35\n",
            "Probability of generating Richard III: 0.5044\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 4: Check if the probability of generating II given RICHARD  are close to the expected values\n",
            "Input:\n",
            " Richard II\n",
            "Probability of generating Richard II:\n",
            " 0.35\n",
            "Expected Probability of generating Richard II:\n",
            " 0.35251798561151076\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 5: Check if the probability of generating III given RICHARD are close to the expected values\n",
            "Input:\n",
            " Richard III\n",
            "Probability of generating Richard III:\n",
            " 0.5044\n",
            "Expected Probability of generating Richard III:\n",
            " 0.49640287769784175\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_sample_text_bigram_model():\n",
        "    bigram_lm = WordNGramLM(2)\n",
        "    bigram_lm.fit(train_data)\n",
        "\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    sampled_text = bigram_lm.sample_text(\"<sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 1: Check if the sampled text starts with <sos>\")\n",
        "    evaluate_test_case(None, sampled_text.startswith(\"<sos>\"), True, output_str=\"Sampled text starts with <sos>\")\n",
        "\n",
        "    print(\"Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\")\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 1}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 1 == 50 or (\n",
        "        len(sampled_text.split()) < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Test Case 3: Check if the probability of generating II is greater than III when prefix is RICHARD\")\n",
        "    sampled_texts = [\n",
        "        bigram_lm.sample_text(\"RICHARD\", max_words=1) for _ in range(10000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"RICHARD\")[1].strip() == \"II\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    num_richard_3s = [\n",
        "        text.split(\"RICHARD\")[1].strip() == \"III\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    gen_prob_richard_2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_richard_3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_richard_2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_richard_3}\")\n",
        "    if gen_prob_richard_2 < gen_prob_richard_3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 4: Check if the probability of generating II given RICHARD  are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\"Richard II\", gen_prob_richard_2, 0.35251798561151076, output_str=\"Probability of generating Richard II\", atol=1e-2)\n",
        "\n",
        "    print(\n",
        "        \"Test Case 5: Check if the probability of generating III given RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\"Richard III\", gen_prob_richard_3, 0.49640287769784175, output_str=\"Probability of generating Richard III\", atol=1e-2)\n",
        "\n",
        "test_sample_text_bigram_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "aAdVznHUa0Qj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0ac3fe-2f8a-4f55-bbd0-c9d90f43e005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sos> KING HENRY VI KING RICHARD : That when for the sun . <eos>\n",
            "<sos> KING EDWARD IV : You bless you disturb devotion shows much corn gratis . <eos>\n",
            "<sos> KING RICHARD III : Why , while a <unk> ; My wretchedness doth not in harvest of dignity and another day . <eos>\n",
            "<sos> KING RICHARD II : yet ? <eos>\n",
            "<sos> KING RICHARD III : 'T is frank 'd A fellow : Not having in the other : <unk> tenderness and crows , That <unk> down I would lay all , I could say this night Whilst that 's son thither : WARWICK : Who , sir , And a person .\n",
            "<sos> KING RICHARD III : I will never would here ? <eos>\n",
            "<sos> KING HENRY VI : this noble kinsman ! <eos>\n",
            "<sos> KING LEWIS XI : what we 'll swear That thou liest ' <eos>\n",
            "<sos> KING HENRY BOLINGBROKE : Do me of Gaunt . <eos>\n",
            "<sos> KING RICHARD III : Amen , my soul and revenge , or any , wretched self , And not speak to them still , I shall pay that sends to time craves to me way : nay . <eos>\n",
            "<sos> KING EDWARD : Why , stay to Saint <unk> but I am no more ; for us , patience , and given his head , It doth approach My lord , the fruit first We have heard , Shall give me ; <unk> of knighthood on it Where all . <eos>\n",
            "<sos> KING RICHARD II : Then were it stay , peace and to Mars , and , Duke of both of office and how his soldiers : when it is done , Even in a complete armour , thou scorn at him in a thing That sought The <unk> <unk> , how\n",
            "<sos> KING EDWARD IV : yet but not little accuse The <unk> will requite this <unk> afflict me how love , adieu . <eos>\n",
            "<sos> KING RICHARD : <unk> And more than a horse . <eos>\n",
            "<sos> KING RICHARD : <unk> , in a man ! <eos>\n",
            "<sos> KING RICHARD II : Now , & <unk> <unk> of Gaunt , here . <eos>\n",
            "<sos> KING HENRY BOLINGBROKE : Richmond ; <unk> or both ? <eos>\n",
            "<sos> KING RICHARD III : Ye 're liars . <eos>\n",
            "<sos> KING LEWIS XI : and fled That dogs bark , too : Marcius gone ; But I protest unto mine <unk> to spend her men , Clifford done with so use that thou art deceived : 'T is no more , I 'll visit you <unk> in request it made Had\n",
            "<sos> KING RICHARD III : Tush , good for length , ' ! <eos>\n"
          ]
        }
      ],
      "source": [
        "for _ in range(20):\n",
        "    sampled_text = bigram_lm.sample_text(\"<sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9FrE4hja0Qk"
      },
      "source": [
        "The generations should look much better now, way more coherent compared to the unigram model! At least on the surface level it seems to capture the style of Shakespeare's writing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E23F-yNna0Qk"
      },
      "source": [
        "Test implementation of `eval_perplexity` for trigram, 4-gram, and 5-gram models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "6NX9vsgya0Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76038853-68e3-4eb3-9c5e-d7a7e19c6f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Perplexity for Trigram model: 5.939635966646537\n",
            "Dev Perplexity for Trigram model: inf\n",
            "\n",
            "\n",
            "\n",
            "Train Perplexity for 4-gram model: 2.0705701088539454\n",
            "Dev Perplexity for 4-gram model: inf\n",
            "\n",
            "\n",
            "\n",
            "Train Perplexity for 5-gram model: 1.595255728780846\n",
            "Dev Perplexity for 5-gram model: inf\n"
          ]
        }
      ],
      "source": [
        "trigram_lm = WordNGramLM(3)\n",
        "trigram_lm.fit(train_data)\n",
        "\n",
        "train_ppl = trigram_lm.eval_perplexity(train_data)\n",
        "dev_ppl = trigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for Trigram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for Trigram model: {dev_ppl}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "fourgram_lm = WordNGramLM(4)\n",
        "fourgram_lm.fit(train_data)\n",
        "train_ppl = fourgram_lm.eval_perplexity(train_data)\n",
        "dev_ppl = fourgram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for 4-gram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for 4-gram model: {dev_ppl}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "fivegram_lm = WordNGramLM(5)\n",
        "fivegram_lm.fit(train_data)\n",
        "train_ppl = fivegram_lm.eval_perplexity(train_data)\n",
        "dev_ppl = fivegram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for 5-gram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for 5-gram model: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER4bx861a0Qk"
      },
      "source": [
        "You should see train perplexities approximately 5.9, 2.1, and 1.6 for trigram, 4-gram, and 5-gram LMs. For dev data the dev perplexity should be infinity for all the three."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldTT7G6na0Qk"
      },
      "source": [
        "Test implementation of `sample_text` for trigram and 4-gram LMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "bo6rr7X_a0Ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc86a676-74bf-4711-fcd5-0b4cc060d504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generations from Trigram model\n",
            "<sos> <sos> KING RICHARD II : I am known to go . <eos>\n",
            "<sos> <sos> KING EDWARD IV : Is it not amiss ; I will to sanctuary . <eos>\n",
            "<sos> <sos> KING RICHARD II : Where <unk> , behold 's : there she lost a brace . <eos>\n",
            "<sos> <sos> KING EDWARD IV : Brave warriors , Clifford , take heed , for Lancaster ! <eos>\n",
            "<sos> <sos> KING HENRY VI : My lord , the fool was that of common soldiers slain . <eos>\n",
            "<sos> <sos> KING EDWARD IV : Go thither ; For this will teach thee how to curse mine enemies ! <eos>\n",
            "<sos> <sos> KING RICHARD III : Say no more hear from you The apprehension of the news abroad ? <eos>\n",
            "<sos> <sos> KING RICHARD II : We have made fault I ' shall poison , go in your city is <unk> 'd out an act of parliament be repeal 'd , Nay , now at my injustice . <eos>\n",
            "<sos> <sos> KING RICHARD III : O Marcius , Attend upon Cominius to these arms , and did scorn an humble tear ; And therefore , not for your country 's foes , to <unk> and <unk> 'd : say it is . <eos>\n",
            "<sos> <sos> KING HENRY VI : Where , he would Have done the time ! <eos>\n",
            "<sos> <sos> KING RICHARD II : Northumberland , his <unk> bride . <eos>\n",
            "<sos> <sos> KING EDWARD IV : Now we have many blasts to shake them ; And where care <unk> , I would not plead , but by Warwick 's other daughter ; till at length have gotten leave To have him hold that purpose and to come to my niece Plantagenet <unk> in\n",
            "<sos> <sos> KING HENRY VI : Be quiet , or next day : It then remains That , to prevent the tyrant from his loins no hopeful branch may spring , I will follow In the main point of death To gaze upon the secrets i n't it had upon its brow A\n",
            "<sos> <sos> KING RICHARD III : Art thou certain this is some changeling : ope n't . <eos>\n",
            "<sos> <sos> KING RICHARD II : So <unk> on a <unk> of warriors , How long is't since ? <eos>\n",
            "<sos> <sos> KING RICHARD III : Come , let 's be brief than tedious . <eos>\n",
            "<sos> <sos> KING HENRY VI : My dagger , and thank you . <eos>\n",
            "<sos> <sos> KING RICHARD III : Ay ; or , by the <unk> . <eos>\n",
            "<sos> <sos> KING EDWARD IV : Why do you think fit to do their amorous rites By their own part , I purpose not to have . <eos>\n",
            "<sos> <sos> KING RICHARD III : Good father , all noble sufferance . <eos>\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "print(\"Generations from Trigram model\")\n",
        "for _ in range(20):\n",
        "    sampled_text = trigram_lm.sample_text(\"<sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "fNyp5YVqa0Ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ff80e3-ad05-4847-d190-2dd4d5cb3857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generations from 4-gram model\n",
            "<sos> <sos> <sos> KING RICHARD II : I am content . <eos>\n",
            "<sos> <sos> <sos> KING RICHARD II : Twice for one step I 'll groan , the way being short , And piece the way out with a heavy heart . <eos>\n",
            "<sos> <sos> <sos> KING HENRY VI : <unk> men , much <unk> with care , Here sits a king more woful than you are hurt by me . <eos>\n",
            "<sos> <sos> <sos> KING RICHARD III : As I remember , this should be the first word of thy speech : For this , being <unk> in sap and blood , your flesh and blood has not offended the king ; and signify to him That thus I have resign 'd my charge to\n",
            "<sos> <sos> <sos> KING RICHARD III : Now , messenger , what letters or what news From France ? <eos>\n",
            "<sos> <sos> <sos> KING HENRY VI : Ay , but mildly . <eos>\n",
            "<sos> <sos> <sos> KING RICHARD III : How chance the prophet could not at that time , <unk> the loving kiss I give the fruit . <eos>\n",
            "<sos> <sos> <sos> KING RICHARD II : A king of beasts , indeed ; and therefore came I hither . <eos>\n",
            "<sos> <sos> <sos> KING EDWARD IV : Hold , Tybalt ! <eos>\n",
            "<sos> <sos> <sos> KING HENRY VI : So flies the <unk> shepherd from the wolf ; So first the harmless sheep doth yield his <unk> And next his throat unto the butcher 's knife . <eos>\n",
            "<sos> <sos> <sos> KING RICHARD III : All unavoided is the doom of destiny . <eos>\n",
            "<sos> <sos> <sos> KING RICHARD II : Northumberland , thou <unk> of an <unk> , <unk> , cook , Both dame and servant ; <unk> all , and left no friendly drop To help me after ? <eos>\n",
            "<sos> <sos> <sos> KING LEWIS XI : And mine with hers , and thine , and Margaret 's . <eos>\n",
            "<sos> <sos> <sos> KING RICHARD II : Ay , by and by , the <unk> of thy sounds , Thou <unk> ! <eos>\n",
            "<sos> <sos> <sos> KING RICHARD III : Why , what is it , but to <unk> myself . <eos>\n",
            "<sos> <sos> <sos> KING EDWARD IV : So other foes may set upon our backs . <eos>\n",
            "<sos> <sos> <sos> KING LEWIS XI : But is this true , sir ? <eos>\n",
            "<sos> <sos> <sos> KING EDWARD IV : But you , my lord : Ely is fled to Richmond ; And Buckingham , back 'd with the <unk> and <unk> of <unk> , Sir John <unk> , Sir <unk> Brakenbury , and will not let me speak ! <eos>\n",
            "<sos> <sos> <sos> KING RICHARD III : Stay , gentle Margaret , and be holp by backward turning ; One desperate grief <unk> with another 's <unk> : Take thou this vial , being then in bed , And this the regal seat , And turn 'd that black word death to banishment :\n",
            "<sos> <sos> <sos> KING EDWARD IV : See that he be convey 'd Unto my brother , <unk> late of <unk> , Of no more soul nor <unk> for the people , Your <unk> <unk> how can he flatter -- That 's thousand to one good one -- when you now see He had\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Generations from 4-gram model\")\n",
        "for _ in range(20):\n",
        "    sampled_text = fourgram_lm.sample_text(\"<sos> <sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "K01hFSaKa0Ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ff0734-08df-45e2-c103-fbc8b5576854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generations from 5-gram model\n",
            "<sos> <sos> <sos> <sos> KING RICHARD II : I am in this , Your wife , your son , these senators , the nobles ; And you will rather show our general <unk> How you can frown than spend a fawn upon 'em , For the inheritance of their loves and safeguard Of what that\n",
            "<sos> <sos> <sos> <sos> KING LEWIS XI : And mine with hers , and thine , and Margaret 's . <eos>\n",
            "<sos> <sos> <sos> <sos> KING EDWARD IV : He 's sudden , if a thing comes in his head . <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD III : Well , sir ; my mistress is the sweetest lady -- Lord , Lord ! <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD III : March on , march on , since we are up in arms ; If not to fight with foreign enemies , Yet to beat down these rebels here at home . <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD II : Twice for one step I 'll groan , the way being short , And piece the way out with a bloody axe . <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD III : Well , let that pass . <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD II : But now the Duke of Buckingham is taken ; That is the best news : that the Earl of Richmond Is with a mighty power landed at <unk> , Is colder tidings , yet they must be told . <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD II : Alack , why am I sent for hither ? <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD II : Thou <unk> me oft for loving Rosaline . <eos>\n",
            "<sos> <sos> <sos> <sos> KING LEWIS XI : Fair Queen of England , worthy Margaret , <unk> down with us : it ill befits thy state And birth , that thou shouldst stand while Lewis doth sit . <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD II : Mine eyes are full of tears , I can not tell : We must proceed as we do find the people . <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD III : Be <unk> in my behalf to her . <eos>\n",
            "<sos> <sos> <sos> <sos> KING LEWIS XI : Welcome , brave Warwick ! <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD III : Be <unk> in my behalf to her . <eos>\n",
            "<sos> <sos> <sos> <sos> KING EDWARD IV : Go to , go to ; You are a saucy boy : is't so , indeed ? <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD III : Up with my tent there ! <eos>\n",
            "<sos> <sos> <sos> <sos> KING RICHARD II : Mowbray , <unk> are our eyes and ears : Were he my brother , nay , my kingdom 's heir , As he is but my father 's brother 's son , Now , by my George , my <unk> , and my approach be shunn 'd\n",
            "<sos> <sos> <sos> <sos> KING RICHARD III : The sun will not be seen to-day ; The sky doth frown and lour upon our army . <eos>\n",
            "<sos> <sos> <sos> <sos> KING HENRY VI : Be patient , for the world is broad and wide . <eos>\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Generations from 5-gram model\")\n",
        "for _ in range(20):\n",
        "    sampled_text = fivegram_lm.sample_text(\"<sos> <sos> <sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "v-Acd2R7m991"
      },
      "outputs": [],
      "source": [
        "# </NO_AUTOGRADE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9kLM-cma0Ql"
      },
      "source": [
        "You should see that the generation quality is now so much better. However, if you look closely (this is specially true for 4-gram and 5-gram models) that some of the sentences are directly lifted from the training data. Which makes sense, as we will increase the order of the N-gram LM, so does we increase its capacity and hence more the ability to memorize its training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orL3WGmma0Qm"
      },
      "source": [
        "## Part 3: Smoothing and Interpolation (27 Points)\n",
        "\n",
        "The issue with using N-gram language models is that any finite training corpus is bound to miss some N-grams that appear in the test set. The models hence assign zero probability to such N-grams, leading to probability of the entire test set to be zero and hence infinite perplexity values that we observed in the previous exercise.\n",
        "\n",
        "The standard way to deal with zero-probability N-gram tokens is to use smoothing algorithms. Smoothing algorithms shave off a bit of probability mass from some more frequent events and give it to unseen events. Smoothing algorithms for N-gram language models is a well studied area of research with numerous algorithms. For this exercise, we will focus on Laplace Smoothing and Interpolation.\n",
        "\n",
        "**Hint:** Try to use the inheritance from `WordNGramLM` to simplify your implementations for this part. You can use `super().method(arguments)` to call a method from the super class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CbTfIF0a0Qm"
      },
      "source": [
        "### Exercise 3.1 Laplace and Add-Lambda Smoothing (10 Points)\n",
        "\n",
        "Perhaps the simplest smoothing algorithm that exists is Laplace smoothing. It merely adds one to count of each N-gram, so that there is no zero-probability N-gram in the test data. For a bigram model, the expression for Laplace-smoothened distribution is given by:\n",
        "\n",
        "$$P_{\\text{Laplace}}(w_n \\mid w_{n-1}) = \\frac{C(w_{n-1}w_{n}) + 1}{\\sum_{w \\in V} (C(w_n w) + 1) } = \\frac{C(w_{n-1}w_{n}) + 1}{ C(w_{n-1}) + |V| }$$\n",
        "\n",
        "We can similarly write expressions for other N-gram models.\n",
        "\n",
        "Laplace smoothing is also called \"Add-one\" smoothing. A generalization of Laplace Smoothing is \"Add-k\" smoothing with  with $\\lambda < 1$, where we move a bit less of the probability mass from seen to unseen N-grams. The expression for Add-k smoothened distribution for bigram LM is given by:\n",
        "\n",
        "$$P_{\\text{Add-k}}(w_n \\mid w_{n -1}) = \\frac{C(w_{n-1}w_{n}) + \\lambda}{\\sum_{w \\in V} (C(w_n w) + \\lambda) } = \\frac{C(w_{n-1}w_{n}) + \\lambda}{ C(w_{n-1}) + \\lambda |V| }$$\n",
        "\n",
        "We will use the variable `k` to represent Lambda in the code.\n",
        "\n",
        "For this exercise, we ask you to implement `WordNGramLMWithAddKSmoothing` class. You need to follow the same code structure as `WordNGramLM` class, with only difference being in calculation of the conditional distributions.\n",
        "\n",
        "Note: It is no longer possible to implement an O(T + V) time complexity solution for `sample_text` function, hence we will accept O(TV) implementations here. Your `eval_perplexity` should still be as efficient as in the `WordNGramLM`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "yFxiq3XLa0Qm",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-34000a508fd844f8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class WordNGramLMWithAddKSmoothing(WordNGramLM):\n",
        "    \"\"\"\n",
        "    Remember you can use the inheritance from WordNGramLM in your implementation!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, N: int, k: int = 1):\n",
        "        super().__init__(N)\n",
        "        self.k = k\n",
        "        self._pred_vocab = []\n",
        "        self._V = 0\n",
        "\n",
        "    def fit(self, train_data: List[str]):\n",
        "        \"\"\"\n",
        "        Trains an N-gram language model with Add-k smoothing.\n",
        "\n",
        "        Inputs:\n",
        "            - train_data: str, sentences in the training data\n",
        "\n",
        "        \"\"\"\n",
        "        # First pass count words to find rare ones\n",
        "        word_counts = defaultdict(int)\n",
        "        for sent in train_data:\n",
        "            sent_processed = self._preprocess_line(sent)\n",
        "            for tok in sent_processed.split():\n",
        "                word_counts[tok] += 1\n",
        "\n",
        "        # Define Vocab\n",
        "        self.vocab = set()\n",
        "        self.vocab.add(\"<unk>\")\n",
        "        self.vocab.add(\"<eos>\")\n",
        "        if self.N > 1:\n",
        "            self.vocab.add(\"<sos>\")\n",
        "\n",
        "        for w, c in word_counts.items():\n",
        "            if c >= 3:\n",
        "                self.vocab.add(w)\n",
        "\n",
        "        # Reset counts\n",
        "        self.context_counts = defaultdict(int)\n",
        "        self.next_counts = defaultdict(lambda: defaultdict(int))\n",
        "        k = self.N - 1\n",
        "\n",
        "        # Second Pass, train\n",
        "        for sent in train_data:\n",
        "            sent_processed = self._preprocess_line(sent)\n",
        "            raw_tokens = sent_processed.split()\n",
        "            tokens = []\n",
        "            for tok in raw_tokens:\n",
        "                if tok in self.vocab:\n",
        "                    tokens.append(tok)\n",
        "                else:\n",
        "                    tokens.append(\"<unk>\")\n",
        "\n",
        "            tokens = self._ensure_sos(tokens)\n",
        "            if self.N == 1:\n",
        "                ctx = tuple()\n",
        "                for w in tokens:\n",
        "                    self.context_counts[ctx] += 1\n",
        "                    self.next_counts[ctx][w] += 1\n",
        "            else:\n",
        "                for i in range(k, len(tokens)):\n",
        "                    ctx = tuple(tokens[i-k:i])\n",
        "                    w = tokens[i]\n",
        "                    self.context_counts[ctx] += 1\n",
        "                    self.next_counts[ctx][w] += 1\n",
        "\n",
        "        # Pre calc prediction vocab for sampling\n",
        "        if self.N > 1:\n",
        "            self._pred_vocab = sorted(list(self.vocab - {\"<sos>\"}))\n",
        "        else:\n",
        "            self._pred_vocab = sorted(list(self.vocab))\n",
        "        self._V = len(self._pred_vocab)\n",
        "\n",
        "        if self._V == 0:\n",
        "            raise ValueError(\"Vocab is empty after fit\")\n",
        "\n",
        "    def eval_perplexity(self, eval_data: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Evaluates the perplexity of the N-gram language model with Add-k smoothing on the eval set.\n",
        "\n",
        "        Input:\n",
        "            - eval_data: List[str], the evaluation text\n",
        "\n",
        "        Output:\n",
        "            - float, the perplexity of the model on the evaluation set\n",
        "\n",
        "        Note : For tokens that are not in the vocabulary, replace them with the <unk> token.\n",
        "\n",
        "        \"\"\"\n",
        "        total_log_prob = 0.0\n",
        "        total_tokens = 0\n",
        "        kctx = self.N - 1\n",
        "        V = self._V\n",
        "        k = self.k\n",
        "        for sent in eval_data:\n",
        "            # hleper preprocessing here\n",
        "            sent_processed = self._preprocess_line(sent)\n",
        "            raw_tokens = sent_processed.split()\n",
        "            raw_tokens = self._ensure_sos(raw_tokens)\n",
        "\n",
        "            tokens = []\n",
        "            for t in raw_tokens:\n",
        "                if t == \"<sos>\":\n",
        "                    tokens.append(t)\n",
        "                else:\n",
        "                    tokens.append(self._map_oov_to_unk(t))\n",
        "\n",
        "            if self.N == 1:\n",
        "                ctx = tuple()\n",
        "                den = self.context_counts.get(ctx, 0) + k * V\n",
        "                for w in tokens:\n",
        "                    num = self.next_counts[ctx].get(w, 0) + k\n",
        "                    total_log_prob += math.log(num) - math.log(den)\n",
        "                    if w != \"<sos>\":\n",
        "                        total_tokens += 1\n",
        "            else:\n",
        "                for i in range(kctx, len(tokens)):\n",
        "                    ctx = tuple(tokens[i-kctx:i])\n",
        "                    w = tokens[i]\n",
        "                    den = self.context_counts.get(ctx, 0) + k * V\n",
        "                    num = self.next_counts[ctx].get(w, 0) + k\n",
        "                    total_log_prob += math.log(num) - math.log(den)\n",
        "                    if w != \"<sos>\":\n",
        "                        total_tokens += 1\n",
        "\n",
        "        if total_tokens == 0:\n",
        "            return 1.0\n",
        "        return math.exp(-total_log_prob / total_tokens)\n",
        "\n",
        "    def sample_text(\n",
        "        self, prefix: str = \"<sos>\", max_words: int = 100,\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Samples text from the N-gram language model.\n",
        "\n",
        "        Inputs:\n",
        "            - prefix: str, the prefix to start the sampling from. Can also be multiple words separated by spaces.\n",
        "            - max_words: int, the maximum number of words to sample\n",
        "\n",
        "        Outputs:\n",
        "            - str, the sampled text\n",
        "\n",
        "        Note: Please use np.random.choice for sampling next words\n",
        "        \"\"\"\n",
        "        if prefix.strip():\n",
        "            prefix_tokens = prefix.strip().split()\n",
        "        else:\n",
        "            prefix_tokens = []\n",
        "        prefix_tokens = self._ensure_sos(prefix_tokens)\n",
        "        prefix_tokens = [t if t == \"<sos>\" else self._map_oov_to_unk(t) for t in prefix_tokens]\n",
        "        out = list(prefix_tokens)\n",
        "        kctx = self.N - 1\n",
        "        k = float(self.k)\n",
        "        pred_vocab = self._pred_vocab\n",
        "        V = self._V\n",
        "\n",
        "        for _ in range(max_words):\n",
        "            if self.N == 1:\n",
        "                ctx = tuple()\n",
        "            else:\n",
        "                ctx = tuple(out[-kctx:])\n",
        "\n",
        "            ctx_count = self.context_counts.get(ctx, 0)\n",
        "            den = ctx_count + k * V\n",
        "            counts_for_ctx = self.next_counts.get(ctx, {})\n",
        "\n",
        "            probs = np.array([(counts_for_ctx.get(w, 0) + k) / den for w in pred_vocab], dtype=float)\n",
        "            probs = probs / probs.sum()\n",
        "\n",
        "            nxt = np.random.choice(pred_vocab, p=probs)\n",
        "            out.append(nxt)\n",
        "            if nxt == \"<eos>\":\n",
        "                break\n",
        "\n",
        "        return \" \".join(out)\n",
        "\n",
        "    # Extra utility functions that you think will be useful can go below\n",
        "\n",
        "    def _preprocess_line(self, line: str) -> str:\n",
        "            line = line.strip()\n",
        "            if not line.endswith(\"<eos>\"):\n",
        "                line += \" <eos>\"\n",
        "            return line\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1B4ipnrPm991"
      },
      "outputs": [],
      "source": [
        "# <NO_AUTOGRADE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqPYhB7la0Qn"
      },
      "source": [
        "Test implementation of `eval_perplexity` for bigram model with Laplace smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "uxgAXJO6a0Qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bee3929-ac28-4d39-b277-1d944fba93a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Perplexity: 403.44737905989643\n",
            "Dev Perplexity: 390.0445447032354\n"
          ]
        }
      ],
      "source": [
        "# Test implementation of `eval_perplexity` for bigram model with Laplace smoothing\n",
        "bigram_lm = WordNGramLMWithAddKSmoothing(2, k = 1)\n",
        "bigram_lm.fit(train_data)\n",
        "train_ppl = bigram_lm.eval_perplexity(train_data)\n",
        "print(f\"Train Perplexity: {train_ppl}\")\n",
        "dev_ppl = bigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Dev Perplexity: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rJnykB2a0Qo"
      },
      "source": [
        "You should get a train perplexity around 404 and dev perplexity around 390. Notice how the dev perplexity is not $\\infty$ anymore! Though it comes at the cost of an increase in perplexity on training data, which is natural since the model now cut offs some probability mass from training N-grams and distribute it to the unseen ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aof4wXE7a0Qo"
      },
      "source": [
        "Test implementation of `eval_perplexity` for bigram model with Add-k smoothing (k = 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "p4Iq_arNa0Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc620701-2708-4264-e5d5-b1fb3c577a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Perplexity: 60.78322177710043\n",
            "Dev Perplexity: 147.92340187733126\n"
          ]
        }
      ],
      "source": [
        "# Test implementation of `eval_perplexity` for bigram model with Add-k smoothing\n",
        "bigram_lm = WordNGramLMWithAddKSmoothing(2, k=0.01)\n",
        "bigram_lm.fit(train_data)\n",
        "train_ppl = bigram_lm.eval_perplexity(train_data)\n",
        "print(f\"Train Perplexity: {train_ppl}\")\n",
        "dev_ppl = bigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Dev Perplexity: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzTuUGj5a0Qp"
      },
      "source": [
        "You should get a train perplexity around 61 and a dev perplexity of roughly 148. Notice how we do much better on train perplexity when k is smaller, since now we re-distribute much less of the mass from the training N-grams. Luckily, in this case it turns out it improves the dev perplexity too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRuwBCtUa0Qp"
      },
      "source": [
        "Test implementation of `sample_text` for bigram model with Laplace Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "fqYM-0vja0Qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4129526e-6a25-439a-f9a0-d353e9f310f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Case 1: Check if the sampled text starts with <sos>\n",
            "Sampled text starts with <sos>:\n",
            " True\n",
            "Expected Sampled text starts with <sos>:\n",
            " True\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\n",
            "Generated text: <sos> LADY waked quickly lords Ross Stands EDWARD succeed lovest prating Antiates whistle south all Tower Volsces chamber hire fancy bustle married RUTLAND butcher defence fools secrets act heavenly live Boy making Truly FLORIZEL vulgar what sith changes LEONTES page fellow My hap Both torment between occasion chose herein innocence Whereto\n",
            "Number of generated words: 50\n",
            "Does the generated text end with <eos>: False\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 3: Check if the probability of generating II is greater than III when prefix is RICHARD\n",
            "Probability of generating Richard II: 0.02\n",
            "Probability of generating Richard III: 0.037\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 4: Check if the probability of generating II given RICHARD  are close to the expected values\n",
            "Input:\n",
            " Richard II\n",
            "Probability of generating Richard II:\n",
            " 0.02\n",
            "Expected Probability of generating Richard II:\n",
            " 0.014\n",
            "Test case failed! :(\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 5: Check if the probability of generating III given RICHARD are close to the expected values\n",
            "Input:\n",
            " Richard III\n",
            "Probability of generating Richard III:\n",
            " 0.037\n",
            "Expected Probability of generating Richard III:\n",
            " 0.034\n",
            "Test case failed! :(\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_sample_text_bigram_laplace_model():\n",
        "    bigram_lm = WordNGramLMWithAddKSmoothing(2, k = 1)\n",
        "    bigram_lm.fit(train_data_wth_unks)\n",
        "\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    sampled_text = bigram_lm.sample_text(\"<sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 1: Check if the sampled text starts with <sos>\")\n",
        "    evaluate_test_case(\n",
        "        None,\n",
        "        sampled_text.startswith(\"<sos>\"),\n",
        "        True,\n",
        "        output_str=\"Sampled text starts with <sos>\",\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\"\n",
        "    )\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 1}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 1 == 50 or (\n",
        "        len(sampled_text.split()) < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 3: Check if the probability of generating II is greater than III when prefix is RICHARD\"\n",
        "    )\n",
        "    sampled_texts = [\n",
        "        bigram_lm.sample_text(\"RICHARD\", max_words=1) for _ in range(1000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"RICHARD\")[1].strip() == \"II\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    num_richard_3s = [\n",
        "        text.split(\"RICHARD\")[1].strip() == \"III\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    gen_prob_richard_2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_richard_3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_richard_2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_richard_3}\")\n",
        "    if gen_prob_richard_2 < gen_prob_richard_3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 4: Check if the probability of generating II given RICHARD  are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"Richard II\",\n",
        "        gen_prob_richard_2,\n",
        "        0.014,\n",
        "        output_str=\"Probability of generating Richard II\",\n",
        "        atol=1e-3,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 5: Check if the probability of generating III given RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"Richard III\",\n",
        "        gen_prob_richard_3,\n",
        "        0.034,\n",
        "        output_str=\"Probability of generating Richard III\",\n",
        "        atol=1e-3,\n",
        "    )\n",
        "\n",
        "test_sample_text_bigram_laplace_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "v9tkvi5ta0Qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9274c55-3b17-432e-9dd7-7b7cd78e849f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sos> KING chose wall quench lose Standing Stanley Doricles summers loves praise Answer whistle south all Tower Volsces chamber hire fancy bustle married RUTLAND butcher defence fools secrets act heavenly live Boy making Truly FLORIZEL vulgar what sith changes LEONTES page fellow My hap Both torment between occasion chose herein innocence Whereto\n",
            "<sos> KING whence sanctuary varlet thou livest truth Hath accomplish Citizens common do blushing songs dare bred infant Return shore GREY worldly sadly accused 'em sisterhood powerful pursues rust GREGORY danger More sunshine might consequence Even clear complete push moveables the great Montgomery prevent ride kindly rush grief hereafter faintly BLUNT Making\n",
            "<sos> KING Boy motion cleft haunt tonight been elder respected arrived Gives bush Than unruly side monster swallow should Worthy thinks imposition shunn thousands coldly Master approved fair sixteen success 'not health envious antic Nor could vice commons her piece deck why weeping begg grows censure broke Bring mar haply DORCAS brain\n",
            "<sos> KING throats bald STANLEY government wood bark others rid bad purged delights monster moon i Hadst spend commanded Whither COMINIUS liege owe Against heed approach napkin Until philosophy dispersed usurp Put court Marry twenty taught betray nuptial slander jewels horn bark Italy threats thumb month could current prunes threats the rude\n",
            "<sos> KING make Heaven That toad majesty 'she Lies obsequies 'banished Tarpeian instruments pick night agree prettiest ay commons remain needy sting notes lack Jack delay blade bawd wild doubt thine mistress serves happiness leads greetings above prosper breathe At near VALERIA velvet warriors tremble departure Accursed understanding fairies whence well stol'n\n",
            "<sos> KING appeal dislike stolen coast To-day joyful using pity lamb Keeper masters would Rest her ta'en rebuke plainly pomp date calm sight siege sung treachery heard hark share new point several these corse destroy James leaves Breathe fray indeed breathing lions Benvolio Brittany smiling daylight POLIXENES heir rue always midnight Her\n",
            "<sos> KING Down hours imprisonment mouth provost wine heigh committed sex bold feeding Grace BISHOP weeps speedily place ear Tullus Sound began intelligence priest nurse bottom wast readiness its marvellous evil beaten dance revel About My Christian Call strength poor garden Know greatest garland Twice fasting drops meat mothers Citizens design mine\n",
            "<sos> KING follow strumpet notwithstanding These Fortune mutiny BUCKINGHAM least veins law dismiss nail foreign ink venture dispatch weeds toads abroad Fifth Left Amen James patience Fifth colours state Against slaves breed Ne'er plain misery talk race shot bribe Vienna repent shroud wounds endured deserves sands counterfeit unkindness strongly fall'n report resolution\n",
            "<sos> KING HENRY thrice harm solely comfortable thoughts dispatch , to London coffin waking walks lark monarch flatterers call confessor ought requite separated senate-house Ireland grieve Draw intended fie their cry Montagues Rome revenges members Left Hector plot From some possible Hail Heaven world defence departure sings voice word reply devise Has\n",
            "<sos> KING resolve keep'st extremity tomb Mars green ; comes Derby Name Nay mutiny relation liar weeping determine brotherhood supply appeal welcome , wilful Castle thieves honest write GREY interchange whisper him many piercing food mischief liberty thy By bride walks thereto foot memory brain Wife fought curses liar God willingly wore\n",
            "<sos> KING on innocence chide sirrah pays Therefore town smiles waken public masters ere unto swords Chertsey BONA devils silence worms Shame loathed discharge whistle stage spit front entertain borne Dorset suit sin youthful yoke joints royalty violence stir beams flock Patrician wars made apt other measure date Mistake ornaments hers sacrifice\n",
            "<sos> KING gain stopp interest kindly takest eagle Pompey BUCKINGHAM repeal merciful possess alas Pray ANTIGONUS cropp line doth fickle to do home seal drew midwife sue waked Second twenty grey betimes forfeit withal gratis conditions moody banish Fortune Part Pale Since ROSS murdering Welshmen craft threaten gates officers Turn You Bring\n",
            "<sos> KING QUEEN bear VIRGILIA Heaven Not fought affect deed hangs persuade CLAUDIO shameless miserable Happy swift trumpet Edward bite shroud remorse Wherefore ages descend goes mend deny forty remembered Bristol behind prey thoughts have imposition Make fled household bastardy blushing diadem Anne commend alive complices No that loved particular sell gust\n",
            "<sos> KING From imposition lightly regreet fan POLIXENES bounty deaths natures lambs dance work mad avoided Lieutenant Since bears Tell Wiltshire brook Ulysses threaten Had ho eight woful Men drop whisper sum slander besides True on undertake judge lambs breast royalty Whose complaint faces haste bade Mowbray many burns lesson Soldier give't\n",
            "<sos> KING good Can corse Plantagenet Earl would condemned sights believe passage rich loathsome gage emulation crown understand sought west Oh quarrel vanish We Exeter rebels latest spy Repair several adulteress They Those sisterhood offend him close tame doubt sitting fees devour forty ceremony remembered haply assured through disinherited inferior tomorrow mildly\n",
            "<sos> KING Hear veins mischance contempt Rather sets merciful humility thither seest Sicilia chivalry because regal Bolingbroke know rot taking courtesy sly May steep POLIXENES dogs shalt Showing arrived prophecy profits must perceive infant begin cracking Welcome took liar dukedom fortnight volume So lineal harmony marr All-Souls swears unrest knight plague turn\n",
            "<sos> KING paradise Six league male extremest rash upper tyrant flies Methinks wonders spoils Our trumpets sure henceforth lips dried Dion continue shorten 'What consul drawn image trow created credit raw flowers appeared foil Return Valeria guilt events tremble dearly lengthen monster ANGELO nourish Vienna wedded Servant else Hadst yon hang long\n",
            "<sos> KING EDWARD rebellion ago threes afeard Wouldst Brother furnish knight Faith sake flock hit few duke joyful Stafford Welcome success visitation destiny boisterous name each BLUNT Stand prisoner number BRAKENBURY ancient ass other ANGELO Isabel sheep Virtue nights backs Lady bachelor prophesy strew sort drew one adversaries caitiff threat ARCHBISHOP Here\n",
            "<sos> KING RICHARD An Well lean exclaims thinking slanderous courteous between dinner lineaments bloody mildness embraced inward fearing canopy vulgar roar Ready suppose govern thou shop fabric April blows incense moon bereft Ready speechless wonderful holding Turn boot All treason Musician laws borne jealousies nice sore affliction ; For tire swifter looking-glass\n",
            "<sos> KING kin offence Tybalt treasons ever discover henceforth Comfort Thou rascals Hast mad battery do't brought dangerous profane careful know gods oak usurp quit alteration Beseech bites lodge DION grow longs contented rust Mab George pure grow perhaps father anon sleeping she pierce book lion deaf Is tribune Pursuivant waking fire\n"
          ]
        }
      ],
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "bigram_lm = WordNGramLMWithAddKSmoothing(2, k = 1)\n",
        "bigram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = bigram_lm.sample_text(\"<sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KrHpXBRa0Qp"
      },
      "source": [
        "You should see generations which are much less like training data! <sos> KING is now followed by words other than name of king names like before. Though the generations are much less clean now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "rSoMzlqda0Qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee4c46d-d675-428f-902d-999ebab86805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sos> KING HENRY raw prime merit Great Isabella , their late milk , will show HENRY BOLINGBROKE : Hold him Although ceremonious lips . <eos>\n",
            "<sos> KING HENRY VI complaint Ye harvest mew 'd isle Hence , when you shall be <unk> me against Warwick Hang All this alteration muse consequence hag helm Repair whereon south unnatural sunshine lief trembles Delphos SCROOP : I fear . <eos>\n",
            "<sos> KING HENRY BOLINGBROKE : Ay comest Could you say , 't please to speak paid Hadst enjoin Had thy majesty Give ROMEO : I say it true bounty Clown cloud retire imagine relation gloves glasses fools ' <eos>\n",
            "<sos> KING EDWARD IV : No quiet actions exceed seal Would I can bite waking sear me to the advantage thieves hear the town amiss Gaoler Page dogs sixteen unrest 're lineal exile Men My fortunes unnatural aunt fitter place ? <eos>\n",
            "<sos> KING lie <unk> at friend . <eos>\n",
            "<sos> KING RICHARD III : He was a <unk> as your country festival relation ah pomp commandment niece mar her . <eos>\n",
            "<sos> KING HENRY BOLINGBROKE : Right noble . <eos>\n",
            "<sos> KING HENRY VI : Therefore approbation unity Our champion Laurence tread treachery better post tire ladyship gracious duke , thus tall man : I say want thereof reverence mightst Friar Laurence to one 'longs Him please , Gaunt drum needless moan Volscians parts Think'st creatures putting on their masters for I\n",
            "<sos> KING HENRY BOLINGBROKE : they follow us hither rent i complexions forlorn Run puissant bring After posterns Saint torn waits upon me ! <eos>\n",
            "<sos> KING LEWIS wandering with this be found shrink common Thank liars unpleasing pains human Hermione inclination worthily Mark hands that spoils please to give him run shut the unjust hanging frowns rivers must not swear to find forward CAPULET : <unk> and matters beaten kneel , <unk> state More Wherein lives\n",
            "<sos> KING RICHARD II : Ah , come gaunt lord ? <eos>\n",
            "<sos> KING RICHARD II deaf as if I 'll wait thyself knew great As I bid me not prize belike voice of my lord , Margaret IV Weep , Art EDWARD : You pay god DUKE OF YORK : Leontes cunning livery loving ANNE : Sometime in this means Cursed George hung\n",
            "<sos> KING EDWARD IV shameless instead enjoys obtain freely leap usurer creature wander through all , And , And so , Think what 's son , As recompense market-place strew remote sense as York haught services worser fell caps royalty brace true sovereign Montagues proud purchase Now the hungry shepherd No princes\n",
            "<sos> KING HENRY BOLINGBROKE pronounced Laurence backs violence vices injustice moans enforcement churchyard by the queen is the <unk> at <unk> error earthly tainted current Gentlemen Cry remedies lies : Against the <unk> stand pregnant Faith , you be false seeming valour withdraw scene cook Cleomenes revenge hated ended trespass Henry and\n",
            "<sos> KING EDWARD IV : And <unk> is so he will deny arguments submission about you 'll warrant , thou hast years , fair without giving liberty mischance downfall of nature supper , Shall we toward him from her <unk> and fly in Corioli west wondering pardons harsh broken seal paid Verily\n",
            "<sos> KING RICHARD III shelter reign knocks does triumph stock : <unk> And this would add more ado were they stumble farewell ; He 's the sentence youthful yielding load reap usurer sport accursed example Great venom manage alteration new made exchange How now at the king more have look upon murders\n",
            "<sos> KING EDWARD : We may not a <unk> ! <eos>\n",
            "<sos> KING RICHARD II : for him as this deed is thine threaten Rivers to-day conceit assured drift wisest flatterers brat manage arms , Berkeley Alone LADY ANNE beads Patience cut the hearing of Warwick , <unk> , Romeo , And <unk> and all dissembling if thou , sir . <eos>\n",
            "<sos> KING RICHARD fine <unk> : Which sounded Vouchsafe Thursday curses forget muster everlasting flint prunes , Must native threat himself appear . <eos>\n",
            "<sos> KING RICHARD II : I 'll give farm charges Mariana tender love him plainly guilt Gives in quiet purple dumb JULIET : I never look down yet methinks My Lord : Go , I can brook thy <unk> caps did yet : ISABELLA tongues tell us : But send twain grandfather\n"
          ]
        }
      ],
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "bigram_lm = WordNGramLMWithAddKSmoothing(2, k=0.01)\n",
        "bigram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = bigram_lm.sample_text(\"<sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNehHdLla0Qq"
      },
      "source": [
        "Notice how with a smaller value of $k$, the generated sentences now start to resemble more with the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dWDvo-pa0Qq"
      },
      "source": [
        "Test implementation of `eval_perplexity` for trigram, 4-gram, and 5-gram LMs with Laplace smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "RLjn8Fn8a0Qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1394ec-6453-48a0-ab1c-2ff1542f75be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Perplexity for Trigram model: 1276.2291220834627\n",
            "Dev Perplexity for Trigram model: 1829.0124278822302\n",
            "\n",
            "\n",
            "\n",
            "Train Perplexity for 4-gram model: 1709.2873613126283\n",
            "Dev Perplexity for 4-gram model: 3040.724184459413\n",
            "\n",
            "\n",
            "\n",
            "Train Perplexity for 5-gram model: 1793.4599605426565\n",
            "Dev Perplexity for 5-gram model: 3337.8334986966984\n"
          ]
        }
      ],
      "source": [
        "trigram_lm = WordNGramLMWithAddKSmoothing(3, k =1)\n",
        "trigram_lm.fit(train_data_wth_unks)\n",
        "\n",
        "train_ppl = trigram_lm.eval_perplexity(train_data_wth_unks)\n",
        "dev_ppl = trigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for Trigram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for Trigram model: {dev_ppl}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "fourgram_lm = WordNGramLMWithAddKSmoothing(4, k =1)\n",
        "fourgram_lm.fit(train_data_wth_unks)\n",
        "train_ppl = fourgram_lm.eval_perplexity(train_data_wth_unks)\n",
        "dev_ppl = fourgram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for 4-gram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for 4-gram model: {dev_ppl}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "fivegram_lm = WordNGramLMWithAddKSmoothing(5, k =1)\n",
        "fivegram_lm.fit(train_data_wth_unks)\n",
        "train_ppl = fivegram_lm.eval_perplexity(train_data_wth_unks)\n",
        "dev_ppl = fivegram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for 5-gram model: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for 5-gram model: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDdtSkNEa0Qq"
      },
      "source": [
        "You should roughly observe the following perplexities:\n",
        "\n",
        "|N-gram LM | Train Perplexity | Dev Perplexity|\n",
        "|----------|------------------|---------------|\n",
        "| Trigram   | 1276              | 1829           |\n",
        "| 4-gram  | 1710              | 3041         |\n",
        "| 5-gram | 1794 | 3339 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK213wVKa0Qq"
      },
      "source": [
        "Test implementation of `sample_text` for trigram and 4-gram LMs with Laplace Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "Gc7WlvFXa0Qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9073956f-efa2-4f40-98c0-691382e52ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for Trigram model\n",
            "Test Case 1: Check if the sampled text starts with <sos> <sos>\n",
            "Sampled text starts with <sos> <sos>:\n",
            " True\n",
            "Expected Sampled text starts with <sos> <sos>:\n",
            " True\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\n",
            "Generated text: <sos> <sos> LADY waked quickly lordship Stands Stands ELBOW summers lovest pray Antigonus whistle sour alliance Welshmen Where changed ho far butchers marry Remember caitiff defy for secure act heavier live Cold manage Troy Faith wait what side changes LEWIS pays felt OVERDONE groan Bound torments between obedient chosen herein innocents While\n",
            "Number of generated words: 50\n",
            "Does the generated text end with <eos>: False\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 3: Check if the probability of generating II is less than III when prefix is KING RICHARD\n",
            "Probability of generating Richard II: 0.0206\n",
            "Probability of generating Richard III: 0.0301\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 4: Check if the probability of generating II given KING RICHARD are close to the expected values\n",
            "Input:\n",
            " King Richard II\n",
            "Probability of generating Richard II:\n",
            " 0.0206\n",
            "Expected Probability of generating Richard II:\n",
            " 0.0201\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 5: Check if the probability of generating III given KING RICHARD are close to the expected values\n",
            "Input:\n",
            " King Richard III\n",
            "Probability of generating Richard III:\n",
            " 0.0301\n",
            "Expected Probability of generating Richard III:\n",
            " 0.0306\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Testing for 4-gram model\n",
            "Test Case 6: Check if the sampled text starts with <sos> <sos> <sos>\n",
            "Sampled text starts with <sos> <sos> <sos>:\n",
            " True\n",
            "Expected Sampled text starts with <sos> <sos> <sos>:\n",
            " True\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 7: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\n",
            "Generated text: <sos> <sos> <sos> head forsake title embraces hoar gently flame salt home coals betimes CAMILLO liar aside Send noble party reputation hard thorn using shut mould chances woman both advancement fiend Down wish likewise joys noon grub Bid marvel prisoner ere returns arguments another Thanks design steel son disdain limbs pronounce bare Time\n",
            "Number of generated words: 50\n",
            "Does the generated text end with <eos>: False\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 8: Check if the probability of generating II is less than III when prefix is <sos> KING RICHARD\n",
            "Probability of generating Richard II: 0.022\n",
            "Probability of generating Richard III: 0.024\n",
            "Test passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 9: Check if the probability of generating II given <sos> KING RICHARD are close to the expected values\n",
            "Input:\n",
            " <sos> King Richard II\n",
            "Probability of generating Richard II:\n",
            " 0.022\n",
            "Expected Probability of generating Richard II:\n",
            " 0.0174\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n",
            "Test Case 10: Check if the probability of generating III given <sos> KING RICHARD are close to the expected values\n",
            "Input:\n",
            " <sos> King Richard III\n",
            "Probability of generating Richard III:\n",
            " 0.024\n",
            "Expected Probability of generating Richard III:\n",
            " 0.0295\n",
            "Test case passed! :)\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_sample_text_ngram_laplace():\n",
        "    print(\"Testing for Trigram model\")\n",
        "\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    trigram_lm = WordNGramLMWithAddKSmoothing(3)\n",
        "    trigram_lm.fit(train_data)\n",
        "    sampled_text = trigram_lm.sample_text(\"<sos> <sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 1: Check if the sampled text starts with <sos> <sos>\")\n",
        "    evaluate_test_case(\n",
        "        None,\n",
        "        sampled_text.startswith(\"<sos> <sos>\"),\n",
        "        True,\n",
        "        output_str=\"Sampled text starts with <sos> <sos>\",\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 2: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\"\n",
        "    )\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 2}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 2 == 50 or (\n",
        "        len(sampled_text.split()) - 2 < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 3: Check if the probability of generating II is less than III when prefix is KING RICHARD\"\n",
        "    )\n",
        "    sampled_texts = [\n",
        "        trigram_lm.sample_text(\"KING RICHARD\", max_words=1) for _ in range(10000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"KING RICHARD\")[1].strip() == \"II\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "    num_richard_3s = [\n",
        "        text.split(\"KING RICHARD\")[1].strip() == \"III\" for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    gen_prob_richard_2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_richard_3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_richard_2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_richard_3}\")\n",
        "    if gen_prob_richard_2 < gen_prob_richard_3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 4: Check if the probability of generating II given KING RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"King Richard II\",\n",
        "        gen_prob_richard_2,\n",
        "        0.0201,\n",
        "        output_str=\"Probability of generating Richard II\",\n",
        "        atol=1e-2,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 5: Check if the probability of generating III given KING RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"King Richard III\",\n",
        "        gen_prob_richard_3,\n",
        "        0.0306,\n",
        "        output_str=\"Probability of generating Richard III\",\n",
        "        atol=1e-2,\n",
        "    )\n",
        "\n",
        "    print(\"Testing for 4-gram model\")\n",
        "    fourgram_lm = WordNGramLMWithAddKSmoothing(4)\n",
        "    fourgram_lm.fit(train_data)\n",
        "    sampled_text = fourgram_lm.sample_text(\"<sos> <sos> <sos>\", max_words=50)\n",
        "\n",
        "    print(\"Test Case 6: Check if the sampled text starts with <sos> <sos> <sos>\")\n",
        "    evaluate_test_case(\n",
        "        None,\n",
        "        sampled_text.startswith(\"<sos> <sos> <sos>\"),\n",
        "        True,\n",
        "        output_str=\"Sampled text starts with <sos> <sos> <sos>\",\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 7: Check if the number of generated words is either 50 or less than 50 and ends with <eos>\"\n",
        "    )\n",
        "    print(f\"Generated text: {sampled_text}\")\n",
        "    print(f\"Number of generated words: {len(sampled_text.split()) - 3}\")\n",
        "    print(f\"Does the generated text end with <eos>: {'<eos>' in sampled_text}\")\n",
        "    if len(sampled_text.split()) - 3 == 50 or (\n",
        "        len(sampled_text.split()) - 3 < 50 and \"<eos>\" in sampled_text\n",
        "    ):\n",
        "        print(\"Test passed! :)\")\n",
        "\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 8: Check if the probability of generating II is less than III when prefix is <sos> KING RICHARD\"\n",
        "    )\n",
        "    sampled_texts = [\n",
        "        fourgram_lm.sample_text(\"<sos> <sos> KING RICHARD\", max_words=1)\n",
        "        for _ in range(1000)\n",
        "    ]\n",
        "    sampled_text = \" \".join(sampled_texts)\n",
        "    num_richard_2s = [\n",
        "        text.split(\"<sos> <sos> KING RICHARD\")[1].strip() == \"II\"\n",
        "        for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    num_richard_3s = [\n",
        "        text.split(\"<sos> <sos> KING RICHARD\")[1].strip() == \"III\"\n",
        "        for text in sampled_texts\n",
        "    ].count(True)\n",
        "\n",
        "    gen_prob_rich2 = num_richard_2s / len(sampled_texts)\n",
        "    gen_prob_rich3 = num_richard_3s / len(sampled_texts)\n",
        "\n",
        "    print(f\"Probability of generating Richard II: {gen_prob_rich2}\")\n",
        "    print(f\"Probability of generating Richard III: {gen_prob_rich3}\")\n",
        "    if gen_prob_rich2 < gen_prob_rich3:\n",
        "        print(\"Test passed! :)\")\n",
        "    else:\n",
        "        print(\"Test failed! :(\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Test Case 9: Check if the probability of generating II given <sos> KING RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"<sos> King Richard II\",\n",
        "        gen_prob_rich2,\n",
        "        0.0174,\n",
        "        output_str=\"Probability of generating Richard II\",\n",
        "        atol=1e-2,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Test Case 10: Check if the probability of generating III given <sos> KING RICHARD are close to the expected values\"\n",
        "    )\n",
        "    evaluate_test_case(\n",
        "        \"<sos> King Richard III\",\n",
        "        gen_prob_rich3,\n",
        "        0.0295,\n",
        "        output_str=\"Probability of generating Richard III\",\n",
        "        atol=1e-2,\n",
        "    )\n",
        "test_sample_text_ngram_laplace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "a7bXoUsEa0Qr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f47ff1-11fb-46fc-d97c-c3694f410c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generations from Trigram model\n",
            "<sos> <sos> KING LEWIS understanding quickly lordship Stands Stands ELBOW summers lovest pray Antigonus whistle sour alliance Welshmen Where changed ho far butchers marry Remember caitiff defy for secure act heavier live Cold manage Troy Faith wait what side changes LEWIS pays felt OVERDONE groan Bound torments between obedient chosen herein innocents While\n",
            "<sos> <sos> KING please salt vast thou'rt loose try I. absolute Clarence complexions division bone sons dangers breaths infant Richmond shop Gentle worldly sad accused 'em sits powers pursuit ruthless Gardener dash Musician sued mightst conquer Exeter choler complexion put move the gods ; pines riches king rushes griefs hills fairest BLUNT March\n",
            "<sos> <sos> KING EDWARD IV : So thus befall empty respected arrived Go bury Thank unhappy sickness moon sway shot Why thinking imperial shut thousands coldly Masters arms fair slave subtle 'not health escape apace Nor corruption vice companion her poor deceit why weeps begins guard centre broken Burgundy mar happier DORCAS bread\n",
            "<sos> <sos> KING RICHARD II : Tell within baseness otherwise rightly back purse deliver monstrous moon idly Into speechless commanding Why Came lion paper Alack hears are natural Unto pestilence disposition usurp RIVERS county Mistake two talk'st betide nurse slander jocund horn base Jerusalem three thumb monument counsel crows public threats the scarcely\n",
            "<sos> <sos> KING RICHARD II : pattern making 'twill Lieutenant occasion 'banished Tewksbury instruments physic night appellant prettiest bachelor complexions relish neither sting notes lace John delights blemish be wild dire think moe sets happier leanness greeting abroad prophet breaths Ay natures Very velvet wars tremble deputy Advance understanding faith whence well storm\n",
            "<sos> <sos> KING HENRY VI ice coast Touching joyful using place lain LARTIUS mean wouldst Repent hence talk'st rebuke plainly policy daughters calumny sight signal sung tread hear'st hang shapes nest poison sex thereof cost devilish Jove leaving Bristol fray infant brows lion Berkeley But smoking dead POMPEY highness ruled am midwife Hercules\n",
            "<sos> <sos> KING EDWARD IV : What one windows heir commonwealth settled boldness fees Greece BLUNT weigh speedy place else Stafford Stay began integrity prime nursed breath wast reach jealousies marry evident became dances revenge About Must Clown Came strength ports gates LEWIS greatly garland Ulysses fasting drowsy meant mothers Claudio determined minister\n",
            "<sos> <sos> KING RICHARD On nourish These Fourth mutiny BUCKINGHAM life veins lay divine nail forest injury venture disperse wedding toads absent Forbear Let Amen Juliet patient Francis come state At signs bribe Ned plague mistress talk'st rank shot bribe Vienna repent shroud wounded enforcement design sat countrymen unknown strongly fall reply resolution\n",
            "<sos> <sos> KING EDWARD orator harm solicit comforts thoughts do't ; thyself Is't comest waking walks last money flatterer calls conference our request royalty senate-house Is't grieves Dukes intelligence fickle theirs cup NORTHUMBERLAND Run rightly melt Lies Hector pluck GEORGE smile powder Happy Henry world determine descent sings voices words rescue devotion He\n",
            "<sos> <sos> KING RICHARD III : mirror Meantime greeting <unk> front Dost Never Nature neighbour rejoice liars weeping detest brotherhood supply appear welcomes AEdile whistle Castle thin honesty write GREY its whisper him metal pity fondly miserable lick thyself Claudio bred walk these footing merit branch Withal found cushion liberal Grace wills wore\n",
            "<sos> <sos> KING RICHARD III : bethink peaceful Therefore town smoking wakes prunes masters even until summers Clarence BRUTUS devour signs worms Shepherd loath direful whit staff spite front entertainment borrow Dost suitor sings youthful yoke join royalty violence stirring beauties flinty Paul wars makes arrest other mellow darken Mistake orphans hers sadly\n",
            "<sos> <sos> KING RICHARD LARTIUS inward kindness taking eagle Prepare Because resort merit possession allow QUEEN Above crying lineal double feather to't cross heavens seas drew middle sudden waked See unborn green betray forgiveness withal greeting conference moon banish Girl Patrician Pardon Since Ravenspurgh music Welshmen craves threatening gate omit Twice York Came\n",
            "<sos> <sos> KING EDWARD IV : Away 'Zounds forsooth affliction decree harbour person CLIFFORD sheep miseries Harp swell trumpets Ely brain shriek remorse Whether agony descend goest melt denied fortunes remember Buckingham beheld prevented though heard how Making flag human bastards blushing didst Another commit alas condemn Nor they lo parlous selfsame guiltless\n",
            "<sos> <sos> KING EDWARD IV : Yes Ulysses Page brittle debt nay lament dancing work majesty baby Live Small beasts Tewksbury Why brother Stand threatening Had hither emulation woful Mercutio drop whisper sullen slain betide Truly only unfold joyful lamp breath ruin William compare faces hated bastard Most marks burning lesson Sometime gladly\n",
            "<sos> <sos> KING RICHARD II : Ay , worthy broke sign beloved pass rheum long future endured crowned ungentle sort west Open queen value Weep Fear receipt laugh stabb Renowned seven admit Think Thinking sister offended him cracking talk doubt skulls fees diadem forty ceremony remember happier assistance throw dishes inferior tomb mildness\n",
            "<sos> <sos> KING EDWARD shake miseries continued Ready service mere humour thither seldom Signior chosen bed refuse Bohemia laid ripe taking courteous smallest Me stealing PRINCE drink shalt Shame articles prophesy profound must pilgrimage infant begins create Welcome tops liars dukes fortunate voices So light harsh marriage Am sweat unrest knight plague tunes\n",
            "<sos> <sos> KING RICHARD II : We forty rate upper ugly flock Mine wonders split Or trumpets surely herald lip drums Dion contract short 'What consul drops ill. trow creatures creeping raze foe appetite foes Richard Valeria guiltless ever tremble death-bed lent monstrous ARCHBISHOP occasion Volsce wedding Servant entertain Hercules yon hap loins\n",
            "<sos> <sos> KING EDWARD Son ah threshold afeard XI Brother gain knife Far salute followers hit festival dukes keys Stafford Welshmen succor visit despised boldly named eleven BLUNT Stanley prison now Bad any aspect others Anon Ludlow shelter Volscian nights bad Lay battle prophesy strengthen sorrows drift once afeard calls threat ARCHBISHOP Here\n",
            "<sos> <sos> KING EDWARD IV : Then estate thinks slanderous court bewray dinner linger blow mildness embrace iron fear canker vulgar roar Repent supply govern those shelter face Arise blunt inclination moon betide Ready special wonderful holds Turn bootless Amen tree Ne'er league both jealous nice sore affliction ; RATCLIFF thrust swift looking-glass\n",
            "<sos> <sos> KING RICHARD III : nourish events disgrace herald Comes Throng reach Hath lurking bearing do't burning dance profane cares knot gentle odd usurping quite alteration Best births lodged DORCAS grown look'st content rushes Made George purse gross perhaps fathers beaten sleeps sheep pines boot lineaments deal Isabella tried RATCLIFF walk first\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Generations from Trigram model\")\n",
        "trigram_lm = WordNGramLMWithAddKSmoothing(3, k = 0.01)\n",
        "trigram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = trigram_lm.sample_text(\"<sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "YfBKbbqCa0Qr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05062e7-c549-48ed-d5ba-45c8cfc14a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generations from 4-gram model with Add-k smoothing\n",
            "<sos> <sos> <sos> KING LEWIS unfold quickly lordship Stands Stands ELBOW summers lovest pray Antigonus whistle sour alliance Welshmen Where changed ho far butchers marry Remember caitiff defy for secure act heavier live Cold manage Troy Faith wait what side changes LEWIS pays felt OVERDONE groan Bound torments between obedient chosen herein innocents While\n",
            "<sos> <sos> <sos> KING please salt vast thou'rt loose try I. absolute Clarence complexions division bone sons dangers breaths infant Richmond shop Gentle worldly sad accused 'em sits powers pursuit ruthless Gardener dash Musician sued mightst conquer Exeter choler complexion put move the gall None prevented riches king rushes griefs hills fairest BLUNT March\n",
            "<sos> <sos> <sos> KING EDWARD IV : So tidings befall empty respected arrived Go bury Thank unhappy sickness moon sway shot Why thinking imperial shut thousands coldly Masters arms fair slave subtle 'not health escape apace Nor corruption vice companion her poor deceit why weeps begins guard centre broken Burgundy mar happier DORCAS bread\n",
            "<sos> <sos> <sos> KING RICHARD II : Tell woo baseness otherwise rightly back purse deliver monstrous moon idly Into speechless commanding Why Came lion paper Alack hears are natural Unto pestilence disposition usurp RIVERS county Mistake two talk'st betide nurse slander jocund horn base Jerusalem three thumb monument counsel crows public threats the scarcely\n",
            "<sos> <sos> <sos> KING RICHARD II : pattern making 'twill Lieutenant occasion 'banished Tewksbury instruments physic night appellant prettiest bachelor complexions relish neither sting notes lace John delights blemish be wild doves think moe sets happier leanness greeting abroad prophet breaths Ay natures Very velvet wars tremble deputy Advance understanding faith whence well storm\n",
            "<sos> <sos> <sos> KING HENRY VI ice coast Touching joyful using place lain LARTIUS mean wouldst Repent hence talk'st rebuke plainly policy daughters calumny sight signal sung tread hear'st hang shapes nest poison sex thereof cost devilish Jove leaving Bristol fray infant brows lion Berkeley But smoking dead POMPEY highness ruled am midwife Hercules\n",
            "<sos> <sos> <sos> KING EDWARD IV : What process wine heir commonwealth settled boldness fees Greece BLUNT weigh speedy place else Ulysses Stay began integrity prime nursed breath wast reach jealousies marry evident became dances revenge About Must Clown Came strength ports gates LEWIS greatly garland Ulysses fasting drowsy meant mothers Claudio determined minister\n",
            "<sos> <sos> <sos> KING RICHARD SURREY nourish These Fourth mutiny BUCKINGHAM life veins lay divine nail forest injury venture disperse wedding toads absent Forbear Let Amen Juliet patient Francis come state At sirs bribe Ned plague mistress talk'st rank shot bribe Vienna repent shroud wounded enforcement design sat countrymen unknown strongly fall reply resolution\n",
            "<sos> <sos> <sos> KING EDWARD power harm solicit comforts thoughts do't ; toads Is't comest waking walks last money flatterer calls conference our request sentence senate-house Is't grieves Dukes intelligence fickle theirs cup NORTHUMBERLAND Run rightly melt Lies Hector pluck GEORGE smile powder Happy Henry world determine descent sings voices words rescue devotion He\n",
            "<sos> <sos> <sos> KING RICHARD III : mirror Meantime greeting <unk> front Dost Never Nature neighbour rejoice liars weeping detest brotherhood supply appear welcomes AEdile whistle Castle thin honesty write GREY its whisper him moan pity fondly miserable lick thyself Claudio bred walk these footing merit branch Withal found cushion liberal Grace wills wore\n",
            "<sos> <sos> <sos> KING RICHARD III : bethink peaceful Therefore town smoking wakes prunes masters even until summers Clarence BRUTUS devour signs worms Shepherd loath direful whit staff spite front entertainment borrow Dost suitor sings youthful yoke join royalty violence stirring beauties flinty Paul wars makes arrest other mellow darken Mistake orphans hers sadly\n",
            "<sos> <sos> <sos> KING RICHARD None inward kindness taking eagle Prepare Because resort merit possession allow QUEEN Above crying lineal double feather to't cross heavens seas drew middle sudden waked See unborn green betray forgiveness withal greeting conference moon banish Girl Patrician Pardon Since Ravenspurgh music Welshmen craves threatening gate omit Twice York Came\n",
            "<sos> <sos> <sos> KING EDWARD IV : Away Make forsooth affliction decree harbour person CLIFFORD sheep miseries Harp swell trumpets Ely brain shriek remorse Whether agony descend goest melt denied fortunes remember Buckingham beheld prevented though heard how Making flag human bastards blushing didst Another commit alas condemn Nor they lo parlous selfsame guiltless\n",
            "<sos> <sos> <sos> KING EDWARD IV : Yes every Page brittle debt nay lament dancing work majesty baby Live Small beasts Tewksbury Why brother Ulysses threatening Had hither emulation woful Mercutio drop whisper sullen slain betide Truly only unfold joyful lamp breath ruin William compare faces hated bastard Most marks burning lesson Sometime gladly\n",
            "<sos> <sos> <sos> KING RICHARD II : Ay Art would committed sign beloved pass rheum long future endured crowned ungentle sort west Open queen value Weep Fear receipt laugh stabb Renowned seven admit Think Thinking sister offended him dastard talk doubt skulls fees diadem forty ceremony remember happier assistance throw dishes inferior tomb mildness\n",
            "<sos> <sos> <sos> KING EDWARD sleep miseries continued Ready service mere humour thither seldom Signior chosen bed refuse Bohemia laid ripe taking courteous smallest Me stealing PRINCE drink shalt Shame articles prophesy profound must pilgrimage infant begins create Welcome tops liars dukes fortunate voices So light harsh marriage Am sweat unrest knight plague tunes\n",
            "<sos> <sos> <sos> KING RICHARD II : We fingers rate upper ugly flock Mine wonders split Or trumpets surely herald lip drums Dion contract short 'What consul drops ill. trow creatures creeping raze foe appetite foes Richard Valeria guiltless ever tremble death-bed lent monstrous ARCHBISHOP occasion Volsce wedding Servant entertain Hercules yon hap loins\n",
            "<sos> <sos> <sos> KING EDWARD beating ah threshold afeard XI Brother gain knife Far salute followers hit festival dukes keys Stafford Welshmen succor visit despised boldly named eleven BLUNT Stanley prison now Bad any aspect others Anon Ludlow shelter Volscian nights bad Lay battle prophesy strengthen sorrows drift once afeard calls threat ARCHBISHOP Here\n",
            "<sos> <sos> <sos> KING EDWARD Ha Welcome lewd exclaims thinks slanderous court bewray dinner linger blow mildness embrace iron fear canker vulgar roar Repent supply govern those shelter face Arise blunt inclination moon betide Ready special wonderful holds Turn bootless Amen tree Ne'er league both jealous nice sore affliction ; RATCLIFF thrust swift looking-glass\n",
            "<sos> <sos> <sos> KING RICHARD III : nourish events disgrace herald Comes Throng reach Hath lurking bearing do't burning dance profane cares knot gentle odd usurping quite alteration Best births lodged DORCAS grown look'st content rushes Made George purse gross perhaps fathers beaten sleeps sheep pines boot lineaments deal Isabella tried RATCLIFF walk first\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Generations from 4-gram model with Add-k smoothing\")\n",
        "fourgram_lm = WordNGramLMWithAddKSmoothing(4, k=0.01)\n",
        "fourgram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = fourgram_lm.sample_text(\"<sos> <sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "Oia_0kuRa0Qr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b4ddfdd-1883-4508-9126-7c98232a5634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generations from 5-gram model with Add-k smoothing\n",
            "<sos> <sos> <sos> <sos> KING LEWIS unfold quickly lordship Stands Stands ELBOW summers lovest pray Antigonus whistle sour alliance Welshmen Where changed ho far butchers marry Remember caitiff defy for secure act heavier live Cold manage Troy Faith wait what side changes LEWIS pays felt OVERDONE groan Bound torments between obedient chosen herein innocents While\n",
            "<sos> <sos> <sos> <sos> KING please salt vast thou'rt loose try I. absolute Clarence complexions division bone sons dangers breaths infant Richmond shop Gentle worldly sad accused 'em sits powers pursuit ruthless Gardener dash Musician sued mightst conquer Exeter choler complexion put move the gall None prevented riches king rushes griefs hills fairest BLUNT March\n",
            "<sos> <sos> <sos> <sos> KING EDWARD IV : So tidings befall empty respected arrived Go bury Thank unhappy sickness moon sway shot Why thinking imperial shut thousands coldly Masters arms fair slave subtle 'not health escape apace Nor corruption vice companion her poor deceit why weeps begins guard centre broken Burgundy mar happier DORCAS bread\n",
            "<sos> <sos> <sos> <sos> KING RICHARD II : Tell woo baseness otherwise rightly back purse deliver monstrous moon idly Into speechless commanding Why Came lion paper Alack hears are natural Unto pestilence disposition usurp RIVERS county Mistake two talk'st betide nurse slander jocund horn base Jerusalem three thumb monument counsel crows public threats the scarcely\n",
            "<sos> <sos> <sos> <sos> KING RICHARD II : pattern making 'twill Lieutenant occasion 'banished Tewksbury instruments physic night appellant prettiest bachelor complexions relish neither sting notes lace John delights blemish be wild doves think moe sets happier leanness greeting abroad prophet breaths Ay natures Very velvet wars tremble deputy Advance understanding faith whence well storm\n",
            "<sos> <sos> <sos> <sos> KING HENRY VI mistress coast Touching joyful using place lain LARTIUS mean wouldst Repent hence talk'st rebuke plainly policy daughters calumny sight signal sung tread hear'st hang shapes nest poison sex thereof cost devilish Jove leaving Bristol fray infant brows lion Berkeley But smoking dead POMPEY highness ruled am midwife Hercules\n",
            "<sos> <sos> <sos> <sos> KING EDWARD IV : What process wine heir commonwealth settled boldness fees Greece BLUNT weigh speedy place else Ulysses Stay began integrity prime nursed breath wast reach jealousies marry evident became dances revenge About Must Clown Came strength ports gates LEWIS greatly garland Ulysses fasting drowsy meant mothers Claudio determined minister\n",
            "<sos> <sos> <sos> <sos> KING RICHARD SURREY nourish These Fourth mutiny BUCKINGHAM life veins lay divine nail forest injury venture disperse wedding toads absent Forbear Let Amen Juliet patient Francis come state At sirs bribe Ned plague mistress talk'st rank shot bribe Vienna repent shroud wounded enforcement design sat countrymen unknown strongly fall reply resolution\n",
            "<sos> <sos> <sos> <sos> KING EDWARD power harm solicit comforts thoughts do't ; toads Is't comest waking walks last money flatterer calls conference our request sentence senate-house Is't grieves Dukes intelligence fickle theirs cup NORTHUMBERLAND Run rightly melt Lies Hector pluck GEORGE smile powder Happy Henry world determine descent sings voices words rescue devotion He\n",
            "<sos> <sos> <sos> <sos> KING RICHARD III : mirror Meantime greeting <unk> front Dost Never Nature neighbour rejoice liars weeping detest brotherhood supply appear welcomes AEdile whistle Castle thin honesty write GREY its whisper him moan pity fondly miserable lick thyself Claudio bred walk these footing merit branch Withal found cushion liberal Grace wills wore\n",
            "<sos> <sos> <sos> <sos> KING RICHARD III : bethink peaceful Therefore town smoking wakes prunes masters even until summers Clarence BRUTUS devour signs worms Shepherd loath direful whit staff spite front entertainment borrow Dost suitor sings youthful yoke join royalty violence stirring beauties flinty Paul wars makes arrest other mellow darken Mistake orphans hers sadly\n",
            "<sos> <sos> <sos> <sos> KING RICHARD None inward kindness taking eagle Prepare Because resort merit possession allow QUEEN Above crying lineal double feather to't cross heavens seas drew middle sudden waked See unborn green betray forgiveness withal greeting conference moon banish Girl Patrician Pardon Since Ravenspurgh music Welshmen craves threatening gate omit Twice York Came\n",
            "<sos> <sos> <sos> <sos> KING EDWARD IV : Away Make forsooth affliction decree harbour person CLIFFORD sheep miseries Harp swell trumpets Ely brain shriek remorse Whether agony descend goest melt denied fortunes remember Buckingham beheld prevented though heard how Making flag human bastards blushing didst Another commit alas condemn Nor they lo parlous selfsame guiltless\n",
            "<sos> <sos> <sos> <sos> KING EDWARD IV : Yes every Page brittle debt nay lament dancing work majesty baby Live Small beasts Tewksbury Why brother Ulysses threatening Had hither emulation woful Mercutio drop whisper sullen slain betide Truly only unfold joyful lamp breath ruin William compare faces hated bastard Most marks burning lesson Sometime gladly\n",
            "<sos> <sos> <sos> <sos> KING RICHARD II : Ay Art would committed sign beloved pass rheum long future endured crowned ungentle sort west Open queen value Weep Fear receipt laugh stabb Renowned seven admit Think Thinking sister offended him dastard talk doubt skulls fees diadem forty ceremony remember happier assistance throw dishes inferior tomb mildness\n",
            "<sos> <sos> <sos> <sos> KING EDWARD sleep miseries continued Ready service mere humour thither seldom Signior chosen bed refuse Bohemia laid ripe taking courteous smallest Me stealing PRINCE drink shalt Shame articles prophesy profound must pilgrimage infant begins create Welcome tops liars dukes fortunate voices So light harsh marriage Am sweat unrest knight plague tunes\n",
            "<sos> <sos> <sos> <sos> KING RICHARD II : We fingers rate upper ugly flock Mine wonders split Or trumpets surely herald lip drums Dion contract short 'What consul drops ill. trow creatures creeping raze foe appetite foes Richard Valeria guiltless ever tremble death-bed lent monstrous ARCHBISHOP occasion Volsce wedding Servant entertain Hercules yon hap loins\n",
            "<sos> <sos> <sos> <sos> KING EDWARD beating ah threshold afeard XI Brother gain knife Far salute followers hit festival dukes keys Stafford Welshmen succor visit despised boldly named eleven BLUNT Stanley prison now Bad any aspect others Anon Ludlow shelter Volscian nights bad Lay battle prophesy strengthen sorrows drift once afeard calls threat ARCHBISHOP Here\n",
            "<sos> <sos> <sos> <sos> KING EDWARD Ha Welcome lewd exclaims thinks slanderous court bewray dinner linger blow mildness embrace iron fear canker vulgar roar Repent supply govern those shelter face Arise blunt inclination moon betide Ready special wonderful holds Turn bootless Amen tree Ne'er league both jealous nice sore affliction ; RATCLIFF thrust swift looking-glass\n",
            "<sos> <sos> <sos> <sos> KING RICHARD III : nourish events disgrace herald Comes Throng reach Hath lurking bearing do't burning dance profane cares knot gentle odd usurping quite alteration Best births lodged DORCAS grown look'st content rushes Made George purse gross perhaps fathers beaten sleeps sheep pines boot lineaments deal Isabella tried RATCLIFF walk first\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Generations from 5-gram model with Add-k smoothing\")\n",
        "fivegram_lm = WordNGramLMWithAddKSmoothing(5, k=0.01)\n",
        "fivegram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = fivegram_lm.sample_text(\"<sos> <sos> <sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "lbrkRzPom992"
      },
      "outputs": [],
      "source": [
        "# </NO_AUTOGRADE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fbCENTKa0Qr"
      },
      "source": [
        "### Write-Up Question 4 (1 Point)\n",
        "\n",
        "Write expression for unigram, trigram, 4-gram, and 5-gram models with Laplace smoothing. (Your answer to this question should go in your separate write-up PDF.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDcHnKVQa0Qr"
      },
      "source": [
        "### Write-Up Question 5 (3 Points)\n",
        "\n",
        "Plot how the train and dev perplexities of bigram, trigram, 4-gram, and 5-gram LMs with Add-k smoothing from different values of k -- $\\{1e-8, 1e-7, \\cdots, 1e-1, 1\\}$. You should have perplexity on the y-axis and k on the x axis. Use log-scaling for the x axis when plotting. Explain the trend that you see in 3 lines. Finally, report the best setup i.e. values of $N$ and $k$ which achieve the best dev accuracy. (Your answer to this question should go in your separate write-up PDF; the following code block below is for the computation you'll need to do in preparation for your write-up.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "APrxWb8zSDcb"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "H7w1drLYa0Qs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# </NO_AUTOGRADE>"
      ],
      "metadata": {
        "id": "dGZixnjpSFC3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKhYDyyVa0Qs"
      },
      "source": [
        "### Exercise 3.3 Language Model Interpolation (10 Points)\n",
        "\n",
        "An alternate to smoothing that often works well in practice is interpolating between different language models. Let's say we are trying to compute $P(w_n \\mid w_{n-2} w_{n-1})$, but we have no examples of the particular trigram $w_{n-2}, w_{n-1} w_n$ in the training corpus, we can instead estimate its probability by using the bigram probability $P(w_n \\mid w_{n-1})$. If there are no examples of the bigram $w_{n-1} w_n$ in the training data either, we use the unigram probability $P(w_n)$. Formally, the trigram probability by mixing the three distributions is given by:\n",
        "\n",
        "$$\\hat{P}(w_n \\mid w_{n-2} w_{n-1}) = \\lambda_1 P(w_n) + \\lambda_2 P(w_n \\mid w_{n-1}) + \\lambda_3 P(w_n \\mid w_{n-1} w_{n-2})$$\n",
        "\n",
        "where $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$ (and each $\\lambda$ is non-negative), making the above equation a form of weighted averaging. We can similarly write expressions for other N-gram LMs.\n",
        "\n",
        "But how do we choose the values of different $\\lambda_i$? We choose these values by tuning them on a held out data i.e. the dev set, very similar to tuning hyperparameters for a machine learning model.\n",
        "\n",
        "In this exercise, you will implement the class `WordNGramLMWithInterpolation` similar to `WordNGramLM` and `WordNGramLMWithAddKSmoothing` that you did in the previous exercises but this time to support interpolation between different LMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "Uv1jZkvEa0Qv",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2bfd0cbf822202bb",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class WordNGramLMWithInterpolation(WordNGramLM):\n",
        "    \"\"\"\n",
        "    Interpolated N-Gram Language Model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, N: int, lambdas: List[float]):\n",
        "        \"\"\"\n",
        "        Constructor for WordNGramLMWithInterpolation class.\n",
        "        Inputs:\n",
        "            - N: int, the N in N-gram\n",
        "            - lambdas: List[float], weights for 1-gram, 2-gram, ... N-gram\n",
        "        \"\"\"\n",
        "        super().__init__(N)\n",
        "        self.lambdas = lambdas\n",
        "        if len(lambdas) != N:\n",
        "            raise ValueError(\"Length of lambdas must match N\")\n",
        "\n",
        "        # Create sub-models\n",
        "        # These inherit from WordNGramLM, so they assume they need to add <eos>\n",
        "        self.sub_models = [WordNGramLM(n) for n in range(1, N + 1)]\n",
        "        self._pred_vocab = []\n",
        "        self._V = 0\n",
        "\n",
        "    def _clean_input(self, data: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Helper: Removes <eos> from the end of input lines if present.\n",
        "        This prevents 'WordNGramLM.fit' from adding a second <eos>,\n",
        "        creating the 'word <eos> <eos>' pattern.\n",
        "        \"\"\"\n",
        "        cleaned = []\n",
        "        for line in data:\n",
        "            line = line.strip()\n",
        "            # If the dataset already has <eos> (like train_data_wth_unks), remove it.\n",
        "            if line.endswith(\"<eos>\"):\n",
        "                line = line[:-5].strip()\n",
        "            cleaned.append(line)\n",
        "        return cleaned\n",
        "\n",
        "    def fit(self, train_data: List[str]):\n",
        "        \"\"\"\n",
        "        Trains an N-gram language model with interpolation.\n",
        "        \"\"\"\n",
        "        # 1. Clean the data (Strip existing <eos>)\n",
        "        # The sub-models' fit() method will add one <eos> back automatically.\n",
        "        clean_data = self._clean_input(train_data)\n",
        "\n",
        "        # 2. Train sub-models on clean data\n",
        "        for model in self.sub_models:\n",
        "            model.fit(clean_data)\n",
        "\n",
        "        # 3. Share vocabulary from the largest model\n",
        "        self.vocab = self.sub_models[-1].vocab\n",
        "\n",
        "        # 4. Prepare prediction vocab\n",
        "        if self.N > 1:\n",
        "            self._pred_vocab = sorted(list(self.vocab - {\"<sos>\"}))\n",
        "        else:\n",
        "            self._pred_vocab = sorted(list(self.vocab))\n",
        "\n",
        "        self._V = len(self._pred_vocab)\n",
        "\n",
        "    def eval_perplexity(self, eval_data: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Evaluates the perplexity of the N-gram language model with interpolation.\n",
        "        \"\"\"\n",
        "        # 1. Clean the input (Strip existing <eos>)\n",
        "        clean_data = self._clean_input(eval_data)\n",
        "\n",
        "        total_log_prob = 0.0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for sent in clean_data:\n",
        "            # 2. Add EXACTLY ONE <eos> for evaluation\n",
        "            sent_with_eos = sent + \" <eos>\"\n",
        "            raw_tokens = sent_with_eos.split()\n",
        "\n",
        "            # Map OOV tokens\n",
        "            tokens = [self._map_oov_to_unk(t) for t in raw_tokens]\n",
        "\n",
        "            # Pad with <sos> for the highest order N\n",
        "            k_max = self.N - 1\n",
        "            padded_tokens = ([\"<sos>\"] * k_max) + tokens\n",
        "\n",
        "            # Iterate over the sequence (including the one <eos>)\n",
        "            for i in range(k_max, len(padded_tokens)):\n",
        "                w = padded_tokens[i]\n",
        "                if w == \"<sos>\": continue\n",
        "\n",
        "                step_prob = 0.0\n",
        "\n",
        "                # Interpolate probabilities\n",
        "                for n_idx, model in enumerate(self.sub_models):\n",
        "                    order = n_idx + 1\n",
        "                    weight = self.lambdas[n_idx]\n",
        "\n",
        "                    k_sub = order - 1\n",
        "                    if k_sub == 0:\n",
        "                        ctx = tuple()\n",
        "                    else:\n",
        "                        ctx = tuple(padded_tokens[i-k_sub : i])\n",
        "\n",
        "                    # Get MLE count-based probability\n",
        "                    count_w = model.next_counts[ctx].get(w, 0)\n",
        "                    count_ctx = model.context_counts.get(ctx, 0)\n",
        "\n",
        "                    if count_ctx > 0:\n",
        "                        p_model = count_w / count_ctx\n",
        "                    else:\n",
        "                        p_model = 0.0\n",
        "\n",
        "                    step_prob += weight * p_model\n",
        "\n",
        "                if step_prob <= 0:\n",
        "                    return float(\"inf\")\n",
        "\n",
        "                total_log_prob += math.log(step_prob)\n",
        "                total_tokens += 1\n",
        "\n",
        "        if total_tokens == 0:\n",
        "            return 1.0\n",
        "\n",
        "        return math.exp(-total_log_prob / total_tokens)\n",
        "\n",
        "    def sample_text(self, prefix: str = \"<sos>\", max_words: int = 100) -> str:\n",
        "        \"\"\"\n",
        "        Samples text from the interpolated model.\n",
        "        \"\"\"\n",
        "        if prefix.strip():\n",
        "            prefix_tokens = prefix.strip().split()\n",
        "        else:\n",
        "            prefix_tokens = []\n",
        "\n",
        "        # Ensure enough SOS tokens\n",
        "        k_max = self.N - 1\n",
        "        lead = 0\n",
        "        while lead < len(prefix_tokens) and prefix_tokens[lead] == \"<sos>\":\n",
        "            lead += 1\n",
        "        prefix_tokens = ([\"<sos>\"] * k_max) + prefix_tokens[lead:]\n",
        "\n",
        "        prefix_tokens = [t if t == \"<sos>\" else self._map_oov_to_unk(t) for t in prefix_tokens]\n",
        "        out = list(prefix_tokens)\n",
        "\n",
        "        pred_vocab = self._pred_vocab\n",
        "        V_size = len(pred_vocab)\n",
        "\n",
        "        for _ in range(max_words):\n",
        "            probs = np.zeros(V_size)\n",
        "\n",
        "            for n_idx, model in enumerate(self.sub_models):\n",
        "                order = n_idx + 1\n",
        "                weight = self.lambdas[n_idx]\n",
        "\n",
        "                k_sub = order - 1\n",
        "                if k_sub == 0:\n",
        "                    ctx = tuple()\n",
        "                else:\n",
        "                    ctx = tuple(out[-k_sub:])\n",
        "\n",
        "                count_ctx = model.context_counts.get(ctx, 0)\n",
        "\n",
        "                if count_ctx > 0:\n",
        "                    # Vectorized probability calculation\n",
        "                    current_counts = np.array([model.next_counts[ctx].get(w, 0) for w in pred_vocab])\n",
        "                    current_probs = current_counts / count_ctx\n",
        "                    probs += weight * current_probs\n",
        "\n",
        "            prob_sum = probs.sum()\n",
        "            if prob_sum == 0:\n",
        "                break\n",
        "            probs = probs / prob_sum\n",
        "\n",
        "            nxt = np.random.choice(pred_vocab, p=probs)\n",
        "            out.append(nxt)\n",
        "            if nxt == \"<eos>\":\n",
        "                break\n",
        "\n",
        "        return \" \".join(out)\n",
        "\n",
        "    def _map_oov_to_unk(self, tok: str) -> str:\n",
        "        return tok if tok in self.vocab else \"<unk>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "wVw2Oea9m992"
      },
      "outputs": [],
      "source": [
        "# <NO_AUTOGRADE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCZ-a3tka0Qv"
      },
      "source": [
        "Test implementation of `eval_perplexity` for trigram model with Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "R300QdoKa0Qv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b884a00c-dafb-4cfc-f629-df3d8b2bf23f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Perplexity for Trigram model with Interpolation: 11.293389655622214\n",
            "Dev Perplexity for Trigram model with Interpolation: 123.68182574518035\n"
          ]
        }
      ],
      "source": [
        "trigram_lm = WordNGramLMWithInterpolation(3, [0.3, 0.3, 0.4])\n",
        "trigram_lm.fit(train_data_wth_unks)\n",
        "\n",
        "train_ppl = trigram_lm.eval_perplexity(train_data_wth_unks)\n",
        "dev_ppl = trigram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for Trigram model with Interpolation: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for Trigram model with Interpolation: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhwkOSWTa0Qv"
      },
      "source": [
        "You should see a train perplexity of around 13 and dev perplexity of 139. This should also be the best performing model based on dev perplexity that we have seen so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vHnynkWa0Qw"
      },
      "source": [
        "Test implementation of `eval_perplexity` for 4-gram model with Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "Vhx4LBa4a0Qw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33480d1-804b-4375-e1a9-02303b270bde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Perplexity for Trigram model with Interpolation: 6.207159821117634\n",
            "Dev Perplexity for Trigram model with Interpolation: 127.21981232925698\n"
          ]
        }
      ],
      "source": [
        "fourgram_lm = WordNGramLMWithInterpolation(4, [0.2, 0.4, 0.3, 0.1])\n",
        "fourgram_lm.fit(train_data_wth_unks)\n",
        "\n",
        "train_ppl = fourgram_lm.eval_perplexity(train_data_wth_unks)\n",
        "dev_ppl = fourgram_lm.eval_perplexity(dev_data)\n",
        "print(f\"Train Perplexity for Trigram model with Interpolation: {train_ppl}\")\n",
        "print(f\"Dev Perplexity for Trigram model with Interpolation: {dev_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj-5Q1Wga0Qw"
      },
      "source": [
        "Here you should get a train perplexity of roughly 8 and dev perplexity around 144."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhCEXY4ca0Qw"
      },
      "source": [
        "Play around with different values of $\\lambda_i$ and see how it effects the train and dev perplexities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "nhjnmHvqa0Qw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe06c390-0ccf-4e4f-ba2e-42a233eac602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generations from Trigram model with Interpolation\n",
            "<sos> <sos> KING HENRY steps in foreign Romeo , : poison from so , with the Bolingbroke , And bear some other York and Edward , and he should KING RICHARD III : Stanley , <eos>\n",
            "<sos> <sos> KING soul usurer her Her <eos>\n",
            "<sos> <sos> KING RICHARD III : Noble . since He did : O Paulina , what serpent that will my wife 's <unk> , Since fate , my good can not <unk> self , word ill Edward ! Once more of thine , Thou <unk> the mind ; <eos>\n",
            "<sos> <sos> KING HENRY VI KING LEWIS him again . <eos>\n",
            "<sos> <sos> KING RICHARD III <unk> balm of my , <eos>\n",
            "<sos> <sos> KING . <unk> <unk> and to be determined he carries <unk> <unk> , vent hence for the right , soft ; one tyranny But , O , that to 's is not Oxford , Vouchsafe to his grave in my wife to The gates are gentlemen ! <eos>\n",
            "<sos> <sos> KING RICHARD : Brother of Gloucester 's death with <unk> my sense . <eos>\n",
            "<sos> <sos> KING HENRY VI Froth Marcius , then ; Ay , in sight ! <eos>\n",
            "<sos> <sos> KING EDWARD IV : What are your <unk> ; <unk> too shallow Italy First if all our as ; then the prince . <eos>\n",
            "<sos> <sos> KING RICHARD is this sentence them , And wander from , <eos>\n",
            "<sos> <sos> KING RICHARD 'd : When she princes . <eos>\n",
            "<sos> <sos> KING EDWARD IV : Though therefore hence amain . <eos>\n",
            "<sos> <sos> KING HENRY Keeper was be these enemies are his life ; Each part , , in Here to to thy desires , there awake your voices of her bride shall forget , away towards To know , my yet : If to kill my man . <eos>\n",
            "<sos> <sos> KING RICHARD Lewis such thing be <unk> of our most me to forget . <eos>\n",
            "<sos> <sos> KING RICHARD . <eos>\n",
            "<sos> <sos> KING RICHARD II : , , something division , O live . <eos>\n",
            "<sos> <sos> KING AEdile -- With instruments upon them weight The heart that a in : , who should live a Richard , I heard the news <unk> were of an he had a bastard by , <unk> 'd , that please some , less noble Richard 's hand and strength , Must\n",
            "<sos> <sos> KING RICHARD III child by Saint <unk> Rutland 's have thy husband , he had in those by worn times ? <eos>\n",
            "<sos> <sos> KING EDWARD , ; have <eos>\n",
            "<sos> <sos> KING HENRY at , the affection . <eos>\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Generations from Trigram model with Interpolation\")\n",
        "trigram_lm = WordNGramLMWithInterpolation(3, [0.3, 0.3, 0.4])\n",
        "trigram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = trigram_lm.sample_text(\"<sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "qkud3ZqVa0Qw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8716817f-e509-4627-9180-baa3eecc27ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generations from 4-gram model with Interpolation\n",
            "<sos> <sos> <sos> KING HENRY of such but <unk> , <unk> their hearts from -- why not FRIAR LAURENCE : His lordship he attain his <unk> <unk> <unk> Upon the Tower . <eos>\n",
            "<sos> <sos> <sos> KING <unk> here , <unk> war was now O , now I <unk> her , those cold ways , and lives . the red with their helps only <eos>\n",
            "<sos> <sos> <sos> KING EDWARD : I do bend their bows ; I , that : you out ; 'd the noble if thou : <unk> , the like a <unk> <eos>\n",
            "<sos> <sos> <sos> KING HENRY VI A the friend , how shall poor humble swain , NORTHUMBERLAND , <unk> 'd man , and to be father had been <eos>\n",
            "<sos> <sos> <sos> KING HENRY BOLINGBROKE living low I thy throat , thou keep'st the stroke Betwixt dread <unk> , she that 's like me . <eos>\n",
            "<sos> <sos> <sos> KING HENRY most assured That ne'er did violence spare enmity . <eos>\n",
            "<sos> <sos> <sos> KING HENRY <eos>\n",
            "<sos> <sos> <sos> KING RICHARD III : Enough then Aufidius , and you ? <eos>\n",
            "<sos> <sos> <sos> KING RICHARD II : I leaden <unk> eye , number . <eos>\n",
            "<sos> <sos> <sos> KING <eos>\n",
            "<sos> <sos> <sos> KING RICHARD III , but a no <unk> malice ; there 's doom , then the crown a soldier : do drop <unk> that we hear fearful king Rome strange one of these -- As to put , <unk> may 's AUMERLE : Unto my brother VI : Have too much\n",
            "<sos> <sos> <sos> KING RICHARD III : <unk> Menenius , Lancaster thee ? by right is the life a bastard by Polixenes , forth <unk> to win the controversy , us happy victory to so betide of the contract I got them in narrow <unk> By your <unk> an opposite to or quickly go\n",
            "<sos> <sos> <sos> KING HENRY VI RICHARD III he hath got the field Should not tell her actions , as . <eos>\n",
            "<sos> <sos> <sos> KING RICHARD II : . , sound as I hear put answer it , : Say how he lads tongue a <unk> of all but <unk> 'd with thee here ; Better be condemn 'd like language <unk> were only his innocence , advise all the , <unk> 'd , take\n",
            "<sos> <sos> <sos> KING RICHARD III : My heart . <eos>\n",
            "<sos> <sos> <sos> KING HENRY VI <unk> 's daughter ; That we may <unk> , grave 's due to me ; but he hath to news which thy brow , And , <unk> league : I think , spur I . <eos>\n",
            "<sos> <sos> <sos> KING RICHARD POMPEY <eos>\n",
            "<sos> <sos> <sos> KING RICHARD II : What says your highness . <eos>\n",
            "<sos> <sos> <sos> KING HENRY is the fool was that ? false aloud as as we drink , the <unk> By yielding water I may counsel My heart is so my mind , and <eos>\n",
            "<sos> <sos> <sos> KING RICHARD III a clamour <eos>\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Generations from 4-gram model with Interpolation\")\n",
        "fourgram_lm = WordNGramLMWithInterpolation(4, [0.2, 0.4, 0.3, 0.1])\n",
        "fourgram_lm.fit(train_data)\n",
        "for _ in range(20):\n",
        "    sampled_text = fourgram_lm.sample_text(\"<sos> <sos> <sos> KING\", max_words=50)\n",
        "    print(sampled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yebBSzHya0Qw"
      },
      "source": [
        "### Write-Up Question 6 (3 Points)\n",
        "\n",
        "For bigram, trigram, and 4-gram models with interpolation, train models with different values of $\\lambda_i$ and evaluate perplexities on train and dev datasets. You should try at least 5 sets of values for each model. Report the $\\lambda_i$ values that you experiment with and train and dev perplexities for each setting and N-gram model. (Your answer to this question should go in your separate write-up PDF; the following code block below is for the computation you'll need to do in preparation for your write-up.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVv5uzW7a0Qx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "147lKMkPm993"
      },
      "outputs": [],
      "source": [
        "# </NO_AUTOGRADE>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "au24-cse-447",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}